{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A4rFH1Ecnieo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import resample\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score,classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cIc13ovufqXo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aId7HU4apl5J",
        "outputId": "3f91a438-d206-4036-f6f2-f9669b907820"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PmoB15KCpH7m"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"C:\\\\Users\\\\UseR\\\\Downloads\\\\Dev_data_to_be_shared.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "LZ8KmoRRp0IL",
        "outputId": "e4bfee16-8452-4ca4-fc66-08e0bb6b7fc0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>account_number</th>\n",
              "      <th>bad_flag</th>\n",
              "      <th>onus_attribute_1</th>\n",
              "      <th>transaction_attribute_1</th>\n",
              "      <th>transaction_attribute_2</th>\n",
              "      <th>transaction_attribute_3</th>\n",
              "      <th>transaction_attribute_4</th>\n",
              "      <th>transaction_attribute_5</th>\n",
              "      <th>transaction_attribute_6</th>\n",
              "      <th>transaction_attribute_7</th>\n",
              "      <th>...</th>\n",
              "      <th>bureau_enquiry_47</th>\n",
              "      <th>bureau_enquiry_48</th>\n",
              "      <th>bureau_enquiry_49</th>\n",
              "      <th>bureau_enquiry_50</th>\n",
              "      <th>onus_attribute_43</th>\n",
              "      <th>onus_attribute_44</th>\n",
              "      <th>onus_attribute_45</th>\n",
              "      <th>onus_attribute_46</th>\n",
              "      <th>onus_attribute_47</th>\n",
              "      <th>onus_attribute_48</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>221000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>25000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>86000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>215000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 1216 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   account_number  bad_flag  onus_attribute_1  transaction_attribute_1  \\\n",
              "0               1         0               NaN                      NaN   \n",
              "1               2         0          221000.0                      0.0   \n",
              "2               3         0           25000.0                      0.0   \n",
              "3               4         0           86000.0                      0.0   \n",
              "4               5         0          215000.0                      0.0   \n",
              "\n",
              "   transaction_attribute_2  transaction_attribute_3  transaction_attribute_4  \\\n",
              "0                      NaN                      NaN                      NaN   \n",
              "1                      0.0                      0.0                      0.0   \n",
              "2                      0.0                      0.0                      0.0   \n",
              "3                      0.0                      0.0                      0.0   \n",
              "4                      0.0                      0.0                      0.0   \n",
              "\n",
              "   transaction_attribute_5  transaction_attribute_6  transaction_attribute_7  \\\n",
              "0                      NaN                      NaN                      NaN   \n",
              "1                      0.0                      0.0                      0.0   \n",
              "2                      0.0                      0.0                      0.0   \n",
              "3                      0.0                      0.0                      0.0   \n",
              "4                      0.0                      0.0                      0.0   \n",
              "\n",
              "   ...  bureau_enquiry_47  bureau_enquiry_48  bureau_enquiry_49  \\\n",
              "0  ...                0.0                0.0                0.0   \n",
              "1  ...                0.0                0.0                2.0   \n",
              "2  ...                0.0                0.0                0.0   \n",
              "3  ...                0.0                0.0                0.0   \n",
              "4  ...                0.0                0.0                0.0   \n",
              "\n",
              "   bureau_enquiry_50  onus_attribute_43  onus_attribute_44  onus_attribute_45  \\\n",
              "0                1.0                NaN                NaN                NaN   \n",
              "1                3.0                0.0                0.0                0.0   \n",
              "2                8.0                NaN                NaN                NaN   \n",
              "3               30.0                NaN                NaN                NaN   \n",
              "4                1.0                NaN                NaN                NaN   \n",
              "\n",
              "   onus_attribute_46  onus_attribute_47  onus_attribute_48  \n",
              "0                NaN                NaN                NaN  \n",
              "1                0.0                0.0                0.0  \n",
              "2                NaN                NaN                NaN  \n",
              "3                NaN                NaN                NaN  \n",
              "4                NaN                NaN                NaN  \n",
              "\n",
              "[5 rows x 1216 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['account_number'].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iLhFe2XvcwAb"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_feature_engineering(df):\n",
        "    # Handling bureau_enquiry\n",
        "    bureau_enq_cols = [col for col in df.columns if col.startswith('bureau_enquiry')]\n",
        "    df['total_enquiries'] = df[bureau_enq_cols].sum(axis=1)\n",
        "    df['avg_enquiries'] = df[bureau_enq_cols].mean(axis=1)\n",
        "    df['max_enquiries'] = df[bureau_enq_cols].max(axis=1)\n",
        "    df['zero_periods'] = (df[bureau_enq_cols] == 0).sum(axis=1)\n",
        "    df['std_dev_enquiries'] = df[bureau_enq_cols].std(axis=1)\n",
        "    df = df.drop(columns=bureau_enq_cols)\n",
        "\n",
        "    # Handling bureau\n",
        "    bureau_cols = [col for col in df.columns if col.startswith('bureau')]\n",
        "    df['bureau_01'] = df[bureau_cols[0:6]].sum(axis=1) + df[bureau_cols[12:35]].sum(axis=1) + df[bureau_cols[66:115]].sum(axis=1)\n",
        "    df['bureau_02'] = df[bureau_cols[6:13]].sum(axis=1) + df[bureau_cols[115:128]].sum(axis=1) + df[bureau_cols[389:409]].sum(axis=1)\n",
        "    df['bureau_03'] = df[bureau_cols[35:67]].sum(axis=1)\n",
        "    df['bureau_04'] = df[bureau_cols[128:388]].sum(axis=1)\n",
        "    df['bureau_05'] = df[bureau_cols[409:453]].sum(axis=1)\n",
        "    df.drop(columns=bureau_cols, inplace=True)\n",
        "\n",
        "    # Handling onus\n",
        "    onus_cols = [col for col in df.columns if col.startswith('onus')]\n",
        "\n",
        "    category_1a = [\n",
        "        'onus_attribute_3', 'onus_attribute_4', 'onus_attribute_10',\n",
        "        'onus_attribute_13', 'onus_attribute_16', 'onus_attribute_18',\n",
        "        'onus_attribute_19', 'onus_attribute_21', 'onus_attribute_22',\n",
        "        'onus_attribute_24'\n",
        "    ]\n",
        "\n",
        "    # Dynamically select columns within the specified range\n",
        "    category_1b = [\n",
        "        col for col in df.columns if col.startswith('onus_attribute_') and\n",
        "        25 <= int(col.split('_')[-1]) <= 42\n",
        "    ]\n",
        "\n",
        "    onus_range_columns = category_1a + category_1b\n",
        "\n",
        "    # Combine both sets of columns\n",
        "    category_1 = onus_range_columns\n",
        "\n",
        "    # Ensure only valid columns are considered (in case some columns are missing from the DataFrame)\n",
        "    onus_feature_1 = [col for col in category_1 if col in df.columns]\n",
        "\n",
        "    category_2 = ['onus_attribute_6','onus_attribute_8','onus_attribute_11','onus_attribute_14']\n",
        "    onus_feature_2 = [col for col in category_2 if col in df.columns]\n",
        "\n",
        "    category_3 = ['onus_attribute_2','onus_attribute_5','onus_attribute_7','onus_attribute_9','onus_attribute_12','onus_attribute_15','onus_attribute_17','onus_attribute_20','onus_attribute_23']\n",
        "    onus_feature_3 = [col for col in category_3 if col in df.columns]\n",
        "\n",
        "    df['onus_attribute_01'] = df[onus_feature_1].sum(axis=1)\n",
        "    df['onus_attribute_02'] = df[onus_feature_2].sum(axis=1)\n",
        "    df['onus_attribute_03'] = df[onus_feature_3].sum(axis=1)\n",
        "    df['onus_attribute_04'] = df[onus_cols[43:49]].sum(axis=1)\n",
        "\n",
        "\n",
        "    df.drop(columns=onus_cols[1:], inplace=True)\n",
        "\n",
        "    # Handling transaction\n",
        "    transaction_attribute = [col for col in df.columns if col.startswith('transaction_attribute')]\n",
        "\n",
        "    # Initialize columns for summed values\n",
        "    df['transaction_col1'] = 0\n",
        "    df['transaction_col2'] = 0\n",
        "    df['transaction_col3'] = 0\n",
        "\n",
        "    # First loop for the range 0 to 117 with step 3\n",
        "    for index in range(0, 117, 3):\n",
        "          # Add values to the transaction columns\n",
        "          df['transaction_col1'] += df[transaction_attribute[index]]\n",
        "          df['transaction_col2'] += df[transaction_attribute[index+1]]\n",
        "          df['transaction_col3'] += df[transaction_attribute[index+2]]\n",
        "\n",
        "    # Second loop for the range 234 to 664 with step 3\n",
        "    for index in range(234, 351, 3):\n",
        "        if index + 3 < len(transaction_attribute) and index + 1 < len(transaction_attribute) and index + 2 < len(transaction_attribute):\n",
        "            # Add values to the transaction columns\n",
        "            df['transaction_col1'] += df[transaction_attribute[index]]\n",
        "            df['transaction_col2'] += df[transaction_attribute[index+1]]\n",
        "            df['transaction_col3'] += df[transaction_attribute[index+2]]\n",
        "\n",
        "    df['transaction_col4'] = df[transaction_attribute[118:235]].sum(axis=1)+df[transaction_attribute[352:665]].sum(axis=1)\n",
        "\n",
        "    df.drop(columns=transaction_attribute, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Standardize numerical features\n",
        "    #scaler = StandardScaler()\n",
        "    #numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "    #numerical_cols = numerical_cols.drop('bad_flag', errors='ignore') #Ignore if 'bad_flag' not present\n",
        "    #df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    print(df.shape)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcgPNlJ9j3Iz",
        "outputId": "e4f34db7-765c-46b7-c247-af34ce480562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(96806, 21)\n"
          ]
        }
      ],
      "source": [
        "df = preprocess_and_feature_engineering(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpCGJFvskGQb"
      },
      "source": [
        "### Drop unnecessory null data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U7hBfnstkFxu"
      },
      "outputs": [],
      "source": [
        "#Drop na values\n",
        "df = df.dropna(subset=['onus_attribute_1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXB56qNzc6RD"
      },
      "source": [
        "### Impute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(71575, 21)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ax2I6Fogcv90"
      },
      "outputs": [],
      "source": [
        "# Create a KNN imputer object\n",
        "imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "\n",
        "# Fit and transform the data\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Now df_imputed contains the DataFrame with imputed NaN values\n",
        "# print(df_imputed.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(71575, 21)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_imputed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGL2TVDSdVE4"
      },
      "source": [
        "### Data Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf25ZJOVcv7Z",
        "outputId": "45595e1b-3bf4-4308-c71d-41c2f24b351e"
      },
      "outputs": [],
      "source": [
        "# from sklearn.utils import resample\n",
        "\n",
        "# # Separate majority and minority classes\n",
        "# df_majority = df_imputed[df_imputed['bad_flag'] == 0]\n",
        "# df_minority = df_imputed[df_imputed['bad_flag'] == 1]\n",
        "\n",
        "# # Upsample minority class to 20% of the majority class size\n",
        "# df_minority_upsampled = resample(df_minority,\n",
        "#                                  replace=True,     # sample with replacement\n",
        "#                                  n_samples=int(len(df_majority) * 0.05),    # 20% of the majority class size\n",
        "#                                  random_state=123) # reproducible results\n",
        "\n",
        "# # Combine majority class with upsampled minority class\n",
        "# df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# # Downsample majority class to match the upsampled minority class size\n",
        "# df_majority_downsampled = resample(df_majority,\n",
        "#                                    replace=False,    # sample without replacement\n",
        "#                                    n_samples=len(df_minority_upsampled),     # to match the upsampled minority class size\n",
        "#                                    random_state=123) # reproducible results\n",
        "\n",
        "# # Combine minority class with downsampled majority class\n",
        "# df_downsampled = pd.concat([df_minority_upsampled, df_majority_downsampled])\n",
        "\n",
        "# # Display new class counts after upsampling and downsampling\n",
        "# # print(\"Upsampled DataFrame class distribution:\")\n",
        "# # print(df_upsampled['bad_flag'].value_counts())\n",
        "\n",
        "# print(\"\\nDownsampled DataFrame class distribution:\")\n",
        "# print(df_downsampled['bad_flag'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3AttxuKywJyp"
      },
      "outputs": [],
      "source": [
        "# !pip install -U imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gPZZc3hwG3B",
        "outputId": "99a54014-bd4c-4178-f955-04d56e1879b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution before SMOTE:\n",
            "bad_flag\n",
            "0.0    70525\n",
            "1.0     1050\n",
            "Name: count, dtype: int64\n",
            "Class distribution after SMOTE:\n",
            "bad_flag\n",
            "0.0    70525\n",
            "1.0    70525\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_imputed.drop('bad_flag', axis=1)  # Drop 'bad_flag' from features\n",
        "y = df_imputed['bad_flag']  # 'bad_flag' is the target column\n",
        "\n",
        "# Apply SMOTE to the whole dataset\n",
        "# Set sampling_strategy as a dictionary to specify the desired number of samples\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Create a new balanced DataFrame\n",
        "df_resampled = pd.DataFrame(X_res, columns=X.columns)\n",
        "df_resampled['bad_flag'] = y_res\n",
        "\n",
        "# Display the class distribution after applying SMOTE\n",
        "print(f\"Class distribution before SMOTE:\\n{y.value_counts()}\")\n",
        "print(f\"Class distribution after SMOTE:\\n{y_res.value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__cjLjwYxj58",
        "outputId": "b3ad24da-9d09-449c-d933-1450c69ae769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(141050, 21)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resampled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OIWsIYiId5Bl"
      },
      "outputs": [],
      "source": [
        "df_upsampled = df_resampled #df_imputed #df_downsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfmem6bpeCtc"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have the original dataframe `df`\n",
        "original_account_numbers = df_upsampled['account_number']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "X = df_upsampled.drop(\"bad_flag\", axis=1)  # Assuming `bad_flag` is the target\n",
        "y = df_upsampled[\"bad_flag\"]\n",
        "\n",
        "# Split data (this is where X_test and X_train are generated)\n",
        "X_train, X_test, y_train, y_test, train_account_numbers, test_account_numbers = train_test_split(\n",
        "    X, y, original_account_numbers, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "94308     65693.840526\n",
              "47231     63868.000000\n",
              "57386     77564.000000\n",
              "124050    79873.342868\n",
              "712         942.000000\n",
              "              ...     \n",
              "110268    41536.327836\n",
              "119879    77012.560758\n",
              "103694    73175.808977\n",
              "131932    48529.000699\n",
              "121958    21651.692620\n",
              "Name: account_number, Length: 112840, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train['account_number']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "94308     65693.840526\n",
              "47231     63868.000000\n",
              "57386     77564.000000\n",
              "124050    79873.342868\n",
              "712         942.000000\n",
              "              ...     \n",
              "110268    41536.327836\n",
              "119879    77012.560758\n",
              "103694    73175.808977\n",
              "131932    48529.000699\n",
              "121958    21651.692620\n",
              "Name: account_number, Length: 112840, dtype: float64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_account_numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(\"account_number\", axis=1))\n",
        "X_test_scaled = scaler.transform(X_test.drop(\"account_number\", axis=1))\n",
        "\n",
        "# Add account_number back to the scaled data if necessary (though it doesn't affect training)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns.drop(\"account_number\"))\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns.drop(\"account_number\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Random Forest\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            " [[13962   272]\n",
            " [  138 13838]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.99     14234\n",
            "         1.0       0.98      0.99      0.99     13976\n",
            "\n",
            "    accuracy                           0.99     28210\n",
            "   macro avg       0.99      0.99      0.99     28210\n",
            "weighted avg       0.99      0.99      0.99     28210\n",
            "\n",
            "Processing: Decision Tree\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            " [[13499   735]\n",
            " [  379 13597]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.95      0.96     14234\n",
            "         1.0       0.95      0.97      0.96     13976\n",
            "\n",
            "    accuracy                           0.96     28210\n",
            "   macro avg       0.96      0.96      0.96     28210\n",
            "weighted avg       0.96      0.96      0.96     28210\n",
            "\n",
            "Test probabilities have been calculated and saved to 'test_predictions.csv'.\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Define a list of classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "}\n",
        "\n",
        "# Dictionary to store the probabilities\n",
        "test_probabilities = {}\n",
        "\n",
        "# Iterate through classifiers\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Processing: {name}\")\n",
        "    \n",
        "    # Train the model on the training data\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predict probabilities for the test dataset\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        y_prob = clf.predict_proba(X_test_scaled)[:, 1]  # Probability of default (class 1)\n",
        "    elif hasattr(clf, \"decision_function\"):\n",
        "        y_prob = clf.decision_function(X_test_scaled)\n",
        "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())  # Normalize to [0, 1]\n",
        "    else:\n",
        "        raise ValueError(f\"Classifier {name} does not support probability predictions.\")\n",
        "    \n",
        "    # Store the probabilities\n",
        "    test_probabilities[name] = y_prob\n",
        "\n",
        "    # Predict the class labels for the test dataset\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\nConfusion Matrix for {name}:\\n\", conf_matrix)\n",
        "    \n",
        "    # Display the confusion matrix using seaborn heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "    plt.title(f'Confusion Matrix for {name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report (optional but helpful for model evaluation)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Choose the best model (e.g., Random Forest based on evaluation metrics)\n",
        "selected_model = \"Random Forest\"  # Change this to the best model after evaluation\n",
        "y_prob_final = test_probabilities[selected_model]\n",
        "\n",
        "# Create a DataFrame to save results using the preserved account numbers\n",
        "test_results = pd.DataFrame({\n",
        "    \"account_number\": test_account_numbers,  # Use the preserved account numbers\n",
        "    \"predicted_probability\": y_prob_final\n",
        "})\n",
        "\n",
        "# Save the results to a CSV file\n",
        "test_results.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Test probabilities have been calculated and saved to 'test_predictions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Predicted Probabilities (first 20 examples):\n",
            "[[0.03 0.97]\n",
            " [0.97 0.03]\n",
            " [0.71 0.29]\n",
            " [0.98 0.02]\n",
            " [0.29 0.71]\n",
            " [1.   0.  ]\n",
            " [0.01 0.99]\n",
            " [0.24 0.76]\n",
            " [1.   0.  ]\n",
            " [0.08 0.92]\n",
            " [0.13 0.87]\n",
            " [1.   0.  ]\n",
            " [0.9  0.1 ]\n",
            " [0.99 0.01]\n",
            " [0.47 0.53]\n",
            " [0.02 0.98]\n",
            " [0.89 0.11]\n",
            " [1.   0.  ]\n",
            " [0.49 0.51]\n",
            " [0.86 0.14]]\n",
            "[0.97 0.03 0.29 ... 0.04 0.16 0.  ]\n",
            "\n",
            "--- Decision Tree ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.99     14234\n",
            "         1.0       0.98      0.99      0.99     13976\n",
            "\n",
            "    accuracy                           0.99     28210\n",
            "   macro avg       0.99      0.99      0.99     28210\n",
            "weighted avg       0.99      0.99      0.99     28210\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "classifiers = {\n",
        "    # \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    # \"XGBoost\": XGBClassifier(),\n",
        "    # \"Support Vector Machine\": SVC(),\n",
        "    # \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=2, class_weight='balanced'),\n",
        "    # \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    # \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "  # Train the classifier\n",
        "  clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Get predicted probabilities\n",
        "  y_pred = clf.predict_proba(X_test_scaled)\n",
        "\n",
        "  # Print out the first 20 predicted probabilities for both classes (0 and 1)\n",
        "  print(\"Full Predicted Probabilities (first 20 examples):\")\n",
        "  print(y_pred[:20])  # Probabilities for both classes\n",
        "\n",
        "  # If you're only interested in the positive class (class 1)\n",
        "  positive_class_probabilities = y_pred[:, 1]\n",
        "  print(positive_class_probabilities)\n",
        "  # Predicted class labels\n",
        "  y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "  # Classification report\n",
        "  print(\"\\n--- Decision Tree ---\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # Confusion Matrix\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(41792, 20)\n"
          ]
        }
      ],
      "source": [
        "validation_data=pd.read_csv(\"C:\\\\Users\\\\UseR\\\\Downloads\\\\validation_data_to_be_shared.csv\")\n",
        "df = preprocess_and_feature_engineering(validation_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate the account_number column before scaling\n",
        "acc_no = df_imputed['account_number']\n",
        "\n",
        "# Drop account_number from df_imputed as it's not part of the features for scaling\n",
        "df_imputed_features = df_imputed.drop(\"account_number\", axis=1)\n",
        "\n",
        "# Apply the scaler to the remaining features\n",
        "df_imputed_scaled = scaler.transform(df_imputed_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\code\\venv\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "d:\\code\\venv\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Assuming `clf` is the trained model\n",
        "y_pred_val = clf.predict(df_imputed_scaled)  # Predict class labels\n",
        "\n",
        "# If you want probabilities instead of class predictions\n",
        "if hasattr(clf, \"predict_proba\"):\n",
        "    y_prob_val = clf.predict_proba(df_imputed_scaled)[:, 1]  # Probability of class 1 (bad_flag=1)\n",
        "else:\n",
        "    y_prob_val = y_pred_val  # If no probabilities, you can just use predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation predictions have been calculated and saved to 'validation_predictions.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Assuming `val_account_numbers` is the list of account numbers corresponding to the validation data\n",
        "validation_results = pd.DataFrame({\n",
        "    \"account_number\": acc_no,  # Account numbers corresponding to validation data\n",
        "    \"predicted_probability\": y_prob_val  # Predicted probabilities or class labels\n",
        "})\n",
        "\n",
        "validation_results.to_csv(\"validation_predictions.csv\", index=False)\n",
        "print(\"Validation predictions have been calculated and saved to 'validation_predictions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\code\\venv\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted probabilities have been calculated and saved to 'validation_data_pred.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Assuming `clf` is the trained model and `df_imputed_scaled` is your test data\n",
        "if hasattr(clf, \"predict_proba\"):\n",
        "    # Get the probabilities for the positive class (bad_flag = 1)\n",
        "    y_prob_val = clf.predict_proba(df_imputed_scaled)[:, 1]  # Select the probability for class 1 (bad_flag=1)\n",
        "else:\n",
        "    raise ValueError(\"The model does not support probability predictions.\")\n",
        "\n",
        "# Create a DataFrame to store account number and predicted probabilities\n",
        "probability_results = pd.DataFrame({\n",
        "    \"account_number\": acc_no,  # Use the preserved account numbers\n",
        "    \"predicted_probability\": y_prob_val  # Probabilities for class 1 (bad_flag=1)\n",
        "})\n",
        "\n",
        "# Save to a CSV file\n",
        "probability_results.to_csv(\"validation_predictions.csv\", index=False)\n",
        "print(\"Predicted probabilities have been calculated and saved to 'validation_data_pred.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnNdnP6of04p"
      },
      "source": [
        "### DLmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Epoch [1/2500], Loss: 0.8398\n",
            "Epoch [2/2500], Loss: 0.7500\n",
            "Epoch [3/2500], Loss: 0.6864\n",
            "Epoch [4/2500], Loss: 0.6497\n",
            "Epoch [5/2500], Loss: 0.6320\n",
            "Epoch [6/2500], Loss: 0.6244\n",
            "Epoch [7/2500], Loss: 0.6212\n",
            "Epoch [8/2500], Loss: 0.6193\n",
            "Epoch [9/2500], Loss: 0.6172\n",
            "Epoch [10/2500], Loss: 0.6143\n",
            "Epoch [11/2500], Loss: 0.6108\n",
            "Epoch [12/2500], Loss: 0.6069\n",
            "Epoch [13/2500], Loss: 0.6030\n",
            "Epoch [14/2500], Loss: 0.5995\n",
            "Epoch [15/2500], Loss: 0.5968\n",
            "Epoch [16/2500], Loss: 0.5949\n",
            "Epoch [17/2500], Loss: 0.5936\n",
            "Epoch [18/2500], Loss: 0.5926\n",
            "Epoch [19/2500], Loss: 0.5918\n",
            "Epoch [20/2500], Loss: 0.5908\n",
            "Epoch [21/2500], Loss: 0.5896\n",
            "Epoch [22/2500], Loss: 0.5882\n",
            "Epoch [23/2500], Loss: 0.5867\n",
            "Epoch [24/2500], Loss: 0.5852\n",
            "Epoch [25/2500], Loss: 0.5838\n",
            "Epoch [26/2500], Loss: 0.5826\n",
            "Epoch [27/2500], Loss: 0.5816\n",
            "Epoch [28/2500], Loss: 0.5806\n",
            "Epoch [29/2500], Loss: 0.5797\n",
            "Epoch [30/2500], Loss: 0.5788\n",
            "Epoch [31/2500], Loss: 0.5779\n",
            "Epoch [32/2500], Loss: 0.5770\n",
            "Epoch [33/2500], Loss: 0.5761\n",
            "Epoch [34/2500], Loss: 0.5752\n",
            "Epoch [35/2500], Loss: 0.5743\n",
            "Epoch [36/2500], Loss: 0.5734\n",
            "Epoch [37/2500], Loss: 0.5726\n",
            "Epoch [38/2500], Loss: 0.5718\n",
            "Epoch [39/2500], Loss: 0.5709\n",
            "Epoch [40/2500], Loss: 0.5700\n",
            "Epoch [41/2500], Loss: 0.5691\n",
            "Epoch [42/2500], Loss: 0.5681\n",
            "Epoch [43/2500], Loss: 0.5672\n",
            "Epoch [44/2500], Loss: 0.5663\n",
            "Epoch [45/2500], Loss: 0.5655\n",
            "Epoch [46/2500], Loss: 0.5647\n",
            "Epoch [47/2500], Loss: 0.5640\n",
            "Epoch [48/2500], Loss: 0.5633\n",
            "Epoch [49/2500], Loss: 0.5626\n",
            "Epoch [50/2500], Loss: 0.5618\n",
            "Epoch [51/2500], Loss: 0.5610\n",
            "Epoch [52/2500], Loss: 0.5601\n",
            "Epoch [53/2500], Loss: 0.5593\n",
            "Epoch [54/2500], Loss: 0.5585\n",
            "Epoch [55/2500], Loss: 0.5577\n",
            "Epoch [56/2500], Loss: 0.5569\n",
            "Epoch [57/2500], Loss: 0.5561\n",
            "Epoch [58/2500], Loss: 0.5553\n",
            "Epoch [59/2500], Loss: 0.5545\n",
            "Epoch [60/2500], Loss: 0.5537\n",
            "Epoch [61/2500], Loss: 0.5529\n",
            "Epoch [62/2500], Loss: 0.5521\n",
            "Epoch [63/2500], Loss: 0.5513\n",
            "Epoch [64/2500], Loss: 0.5505\n",
            "Epoch [65/2500], Loss: 0.5497\n",
            "Epoch [66/2500], Loss: 0.5489\n",
            "Epoch [67/2500], Loss: 0.5481\n",
            "Epoch [68/2500], Loss: 0.5473\n",
            "Epoch [69/2500], Loss: 0.5464\n",
            "Epoch [70/2500], Loss: 0.5456\n",
            "Epoch [71/2500], Loss: 0.5448\n",
            "Epoch [72/2500], Loss: 0.5440\n",
            "Epoch [73/2500], Loss: 0.5431\n",
            "Epoch [74/2500], Loss: 0.5423\n",
            "Epoch [75/2500], Loss: 0.5415\n",
            "Epoch [76/2500], Loss: 0.5407\n",
            "Epoch [77/2500], Loss: 0.5399\n",
            "Epoch [78/2500], Loss: 0.5391\n",
            "Epoch [79/2500], Loss: 0.5383\n",
            "Epoch [80/2500], Loss: 0.5374\n",
            "Epoch [81/2500], Loss: 0.5366\n",
            "Epoch [82/2500], Loss: 0.5358\n",
            "Epoch [83/2500], Loss: 0.5350\n",
            "Epoch [84/2500], Loss: 0.5342\n",
            "Epoch [85/2500], Loss: 0.5333\n",
            "Epoch [86/2500], Loss: 0.5325\n",
            "Epoch [87/2500], Loss: 0.5317\n",
            "Epoch [88/2500], Loss: 0.5309\n",
            "Epoch [89/2500], Loss: 0.5300\n",
            "Epoch [90/2500], Loss: 0.5292\n",
            "Epoch [91/2500], Loss: 0.5284\n",
            "Epoch [92/2500], Loss: 0.5275\n",
            "Epoch [93/2500], Loss: 0.5267\n",
            "Epoch [94/2500], Loss: 0.5258\n",
            "Epoch [95/2500], Loss: 0.5250\n",
            "Epoch [96/2500], Loss: 0.5242\n",
            "Epoch [97/2500], Loss: 0.5233\n",
            "Epoch [98/2500], Loss: 0.5225\n",
            "Epoch [99/2500], Loss: 0.5216\n",
            "Epoch [100/2500], Loss: 0.5208\n",
            "Epoch [101/2500], Loss: 0.5199\n",
            "Epoch [102/2500], Loss: 0.5190\n",
            "Epoch [103/2500], Loss: 0.5182\n",
            "Epoch [104/2500], Loss: 0.5173\n",
            "Epoch [105/2500], Loss: 0.5164\n",
            "Epoch [106/2500], Loss: 0.5156\n",
            "Epoch [107/2500], Loss: 0.5147\n",
            "Epoch [108/2500], Loss: 0.5138\n",
            "Epoch [109/2500], Loss: 0.5129\n",
            "Epoch [110/2500], Loss: 0.5120\n",
            "Epoch [111/2500], Loss: 0.5111\n",
            "Epoch [112/2500], Loss: 0.5102\n",
            "Epoch [113/2500], Loss: 0.5093\n",
            "Epoch [114/2500], Loss: 0.5085\n",
            "Epoch [115/2500], Loss: 0.5076\n",
            "Epoch [116/2500], Loss: 0.5067\n",
            "Epoch [117/2500], Loss: 0.5058\n",
            "Epoch [118/2500], Loss: 0.5049\n",
            "Epoch [119/2500], Loss: 0.5040\n",
            "Epoch [120/2500], Loss: 0.5031\n",
            "Epoch [121/2500], Loss: 0.5022\n",
            "Epoch [122/2500], Loss: 0.5013\n",
            "Epoch [123/2500], Loss: 0.5003\n",
            "Epoch [124/2500], Loss: 0.4994\n",
            "Epoch [125/2500], Loss: 0.4985\n",
            "Epoch [126/2500], Loss: 0.4976\n",
            "Epoch [127/2500], Loss: 0.4967\n",
            "Epoch [128/2500], Loss: 0.4958\n",
            "Epoch [129/2500], Loss: 0.4949\n",
            "Epoch [130/2500], Loss: 0.4940\n",
            "Epoch [131/2500], Loss: 0.4930\n",
            "Epoch [132/2500], Loss: 0.4921\n",
            "Epoch [133/2500], Loss: 0.4912\n",
            "Epoch [134/2500], Loss: 0.4902\n",
            "Epoch [135/2500], Loss: 0.4893\n",
            "Epoch [136/2500], Loss: 0.4884\n",
            "Epoch [137/2500], Loss: 0.4874\n",
            "Epoch [138/2500], Loss: 0.4865\n",
            "Epoch [139/2500], Loss: 0.4855\n",
            "Epoch [140/2500], Loss: 0.4846\n",
            "Epoch [141/2500], Loss: 0.4836\n",
            "Epoch [142/2500], Loss: 0.4827\n",
            "Epoch [143/2500], Loss: 0.4817\n",
            "Epoch [144/2500], Loss: 0.4807\n",
            "Epoch [145/2500], Loss: 0.4798\n",
            "Epoch [146/2500], Loss: 0.4788\n",
            "Epoch [147/2500], Loss: 0.4778\n",
            "Epoch [148/2500], Loss: 0.4769\n",
            "Epoch [149/2500], Loss: 0.4759\n",
            "Epoch [150/2500], Loss: 0.4749\n",
            "Epoch [151/2500], Loss: 0.4739\n",
            "Epoch [152/2500], Loss: 0.4729\n",
            "Epoch [153/2500], Loss: 0.4720\n",
            "Epoch [154/2500], Loss: 0.4710\n",
            "Epoch [155/2500], Loss: 0.4700\n",
            "Epoch [156/2500], Loss: 0.4690\n",
            "Epoch [157/2500], Loss: 0.4680\n",
            "Epoch [158/2500], Loss: 0.4670\n",
            "Epoch [159/2500], Loss: 0.4661\n",
            "Epoch [160/2500], Loss: 0.4651\n",
            "Epoch [161/2500], Loss: 0.4643\n",
            "Epoch [162/2500], Loss: 0.4643\n",
            "Epoch [163/2500], Loss: 0.4655\n",
            "Epoch [164/2500], Loss: 0.4629\n",
            "Epoch [165/2500], Loss: 0.4606\n",
            "Epoch [166/2500], Loss: 0.4614\n",
            "Epoch [167/2500], Loss: 0.4596\n",
            "Epoch [168/2500], Loss: 0.4583\n",
            "Epoch [169/2500], Loss: 0.4586\n",
            "Epoch [170/2500], Loss: 0.4567\n",
            "Epoch [171/2500], Loss: 0.4561\n",
            "Epoch [172/2500], Loss: 0.4559\n",
            "Epoch [173/2500], Loss: 0.4542\n",
            "Epoch [174/2500], Loss: 0.4540\n",
            "Epoch [175/2500], Loss: 0.4534\n",
            "Epoch [176/2500], Loss: 0.4519\n",
            "Epoch [177/2500], Loss: 0.4517\n",
            "Epoch [178/2500], Loss: 0.4510\n",
            "Epoch [179/2500], Loss: 0.4496\n",
            "Epoch [180/2500], Loss: 0.4494\n",
            "Epoch [181/2500], Loss: 0.4487\n",
            "Epoch [182/2500], Loss: 0.4474\n",
            "Epoch [183/2500], Loss: 0.4471\n",
            "Epoch [184/2500], Loss: 0.4465\n",
            "Epoch [185/2500], Loss: 0.4452\n",
            "Epoch [186/2500], Loss: 0.4448\n",
            "Epoch [187/2500], Loss: 0.4444\n",
            "Epoch [188/2500], Loss: 0.4432\n",
            "Epoch [189/2500], Loss: 0.4424\n",
            "Epoch [190/2500], Loss: 0.4421\n",
            "Epoch [191/2500], Loss: 0.4412\n",
            "Epoch [192/2500], Loss: 0.4402\n",
            "Epoch [193/2500], Loss: 0.4396\n",
            "Epoch [194/2500], Loss: 0.4391\n",
            "Epoch [195/2500], Loss: 0.4384\n",
            "Epoch [196/2500], Loss: 0.4374\n",
            "Epoch [197/2500], Loss: 0.4368\n",
            "Epoch [198/2500], Loss: 0.4363\n",
            "Epoch [199/2500], Loss: 0.4356\n",
            "Epoch [200/2500], Loss: 0.4348\n",
            "Epoch [201/2500], Loss: 0.4340\n",
            "Epoch [202/2500], Loss: 0.4333\n",
            "Epoch [203/2500], Loss: 0.4327\n",
            "Epoch [204/2500], Loss: 0.4321\n",
            "Epoch [205/2500], Loss: 0.4316\n",
            "Epoch [206/2500], Loss: 0.4309\n",
            "Epoch [207/2500], Loss: 0.4303\n",
            "Epoch [208/2500], Loss: 0.4296\n",
            "Epoch [209/2500], Loss: 0.4289\n",
            "Epoch [210/2500], Loss: 0.4281\n",
            "Epoch [211/2500], Loss: 0.4275\n",
            "Epoch [212/2500], Loss: 0.4268\n",
            "Epoch [213/2500], Loss: 0.4262\n",
            "Epoch [214/2500], Loss: 0.4256\n",
            "Epoch [215/2500], Loss: 0.4252\n",
            "Epoch [216/2500], Loss: 0.4249\n",
            "Epoch [217/2500], Loss: 0.4251\n",
            "Epoch [218/2500], Loss: 0.4250\n",
            "Epoch [219/2500], Loss: 0.4247\n",
            "Epoch [220/2500], Loss: 0.4227\n",
            "Epoch [221/2500], Loss: 0.4211\n",
            "Epoch [222/2500], Loss: 0.4206\n",
            "Epoch [223/2500], Loss: 0.4209\n",
            "Epoch [224/2500], Loss: 0.4209\n",
            "Epoch [225/2500], Loss: 0.4196\n",
            "Epoch [226/2500], Loss: 0.4182\n",
            "Epoch [227/2500], Loss: 0.4177\n",
            "Epoch [228/2500], Loss: 0.4177\n",
            "Epoch [229/2500], Loss: 0.4176\n",
            "Epoch [230/2500], Loss: 0.4166\n",
            "Epoch [231/2500], Loss: 0.4154\n",
            "Epoch [232/2500], Loss: 0.4148\n",
            "Epoch [233/2500], Loss: 0.4146\n",
            "Epoch [234/2500], Loss: 0.4145\n",
            "Epoch [235/2500], Loss: 0.4139\n",
            "Epoch [236/2500], Loss: 0.4131\n",
            "Epoch [237/2500], Loss: 0.4122\n",
            "Epoch [238/2500], Loss: 0.4115\n",
            "Epoch [239/2500], Loss: 0.4111\n",
            "Epoch [240/2500], Loss: 0.4108\n",
            "Epoch [241/2500], Loss: 0.4106\n",
            "Epoch [242/2500], Loss: 0.4101\n",
            "Epoch [243/2500], Loss: 0.4096\n",
            "Epoch [244/2500], Loss: 0.4089\n",
            "Epoch [245/2500], Loss: 0.4081\n",
            "Epoch [246/2500], Loss: 0.4074\n",
            "Epoch [247/2500], Loss: 0.4068\n",
            "Epoch [248/2500], Loss: 0.4062\n",
            "Epoch [249/2500], Loss: 0.4057\n",
            "Epoch [250/2500], Loss: 0.4053\n",
            "Epoch [251/2500], Loss: 0.4049\n",
            "Epoch [252/2500], Loss: 0.4046\n",
            "Epoch [253/2500], Loss: 0.4046\n",
            "Epoch [254/2500], Loss: 0.4053\n",
            "Epoch [255/2500], Loss: 0.4061\n",
            "Epoch [256/2500], Loss: 0.4072\n",
            "Epoch [257/2500], Loss: 0.4045\n",
            "Epoch [258/2500], Loss: 0.4017\n",
            "Epoch [259/2500], Loss: 0.4008\n",
            "Epoch [260/2500], Loss: 0.4019\n",
            "Epoch [261/2500], Loss: 0.4027\n",
            "Epoch [262/2500], Loss: 0.4006\n",
            "Epoch [263/2500], Loss: 0.3989\n",
            "Epoch [264/2500], Loss: 0.3989\n",
            "Epoch [265/2500], Loss: 0.3994\n",
            "Epoch [266/2500], Loss: 0.3991\n",
            "Epoch [267/2500], Loss: 0.3974\n",
            "Epoch [268/2500], Loss: 0.3966\n",
            "Epoch [269/2500], Loss: 0.3968\n",
            "Epoch [270/2500], Loss: 0.3967\n",
            "Epoch [271/2500], Loss: 0.3961\n",
            "Epoch [272/2500], Loss: 0.3951\n",
            "Epoch [273/2500], Loss: 0.3945\n",
            "Epoch [274/2500], Loss: 0.3942\n",
            "Epoch [275/2500], Loss: 0.3940\n",
            "Epoch [276/2500], Loss: 0.3937\n",
            "Epoch [277/2500], Loss: 0.3931\n",
            "Epoch [278/2500], Loss: 0.3924\n",
            "Epoch [279/2500], Loss: 0.3917\n",
            "Epoch [280/2500], Loss: 0.3912\n",
            "Epoch [281/2500], Loss: 0.3911\n",
            "Epoch [282/2500], Loss: 0.3909\n",
            "Epoch [283/2500], Loss: 0.3905\n",
            "Epoch [284/2500], Loss: 0.3900\n",
            "Epoch [285/2500], Loss: 0.3895\n",
            "Epoch [286/2500], Loss: 0.3891\n",
            "Epoch [287/2500], Loss: 0.3886\n",
            "Epoch [288/2500], Loss: 0.3880\n",
            "Epoch [289/2500], Loss: 0.3874\n",
            "Epoch [290/2500], Loss: 0.3870\n",
            "Epoch [291/2500], Loss: 0.3867\n",
            "Epoch [292/2500], Loss: 0.3862\n",
            "Epoch [293/2500], Loss: 0.3859\n",
            "Epoch [294/2500], Loss: 0.3856\n",
            "Epoch [295/2500], Loss: 0.3858\n",
            "Epoch [296/2500], Loss: 0.3864\n",
            "Epoch [297/2500], Loss: 0.3883\n",
            "Epoch [298/2500], Loss: 0.3893\n",
            "Epoch [299/2500], Loss: 0.3903\n",
            "Epoch [300/2500], Loss: 0.3853\n",
            "Epoch [301/2500], Loss: 0.3823\n",
            "Epoch [302/2500], Loss: 0.3830\n",
            "Epoch [303/2500], Loss: 0.3848\n",
            "Epoch [304/2500], Loss: 0.3850\n",
            "Epoch [305/2500], Loss: 0.3815\n",
            "Epoch [306/2500], Loss: 0.3805\n",
            "Epoch [307/2500], Loss: 0.3820\n",
            "Epoch [308/2500], Loss: 0.3820\n",
            "Epoch [309/2500], Loss: 0.3805\n",
            "Epoch [310/2500], Loss: 0.3789\n",
            "Epoch [311/2500], Loss: 0.3791\n",
            "Epoch [312/2500], Loss: 0.3799\n",
            "Epoch [313/2500], Loss: 0.3791\n",
            "Epoch [314/2500], Loss: 0.3779\n",
            "Epoch [315/2500], Loss: 0.3771\n",
            "Epoch [316/2500], Loss: 0.3770\n",
            "Epoch [317/2500], Loss: 0.3773\n",
            "Epoch [318/2500], Loss: 0.3769\n",
            "Epoch [319/2500], Loss: 0.3761\n",
            "Epoch [320/2500], Loss: 0.3751\n",
            "Epoch [321/2500], Loss: 0.3748\n",
            "Epoch [322/2500], Loss: 0.3749\n",
            "Epoch [323/2500], Loss: 0.3748\n",
            "Epoch [324/2500], Loss: 0.3743\n",
            "Epoch [325/2500], Loss: 0.3736\n",
            "Epoch [326/2500], Loss: 0.3730\n",
            "Epoch [327/2500], Loss: 0.3725\n",
            "Epoch [328/2500], Loss: 0.3722\n",
            "Epoch [329/2500], Loss: 0.3719\n",
            "Epoch [330/2500], Loss: 0.3717\n",
            "Epoch [331/2500], Loss: 0.3717\n",
            "Epoch [332/2500], Loss: 0.3714\n",
            "Epoch [333/2500], Loss: 0.3713\n",
            "Epoch [334/2500], Loss: 0.3709\n",
            "Epoch [335/2500], Loss: 0.3707\n",
            "Epoch [336/2500], Loss: 0.3704\n",
            "Epoch [337/2500], Loss: 0.3702\n",
            "Epoch [338/2500], Loss: 0.3698\n",
            "Epoch [339/2500], Loss: 0.3695\n",
            "Epoch [340/2500], Loss: 0.3689\n",
            "Epoch [341/2500], Loss: 0.3687\n",
            "Epoch [342/2500], Loss: 0.3681\n",
            "Epoch [343/2500], Loss: 0.3677\n",
            "Epoch [344/2500], Loss: 0.3671\n",
            "Epoch [345/2500], Loss: 0.3666\n",
            "Epoch [346/2500], Loss: 0.3661\n",
            "Epoch [347/2500], Loss: 0.3658\n",
            "Epoch [348/2500], Loss: 0.3655\n",
            "Epoch [349/2500], Loss: 0.3653\n",
            "Epoch [350/2500], Loss: 0.3650\n",
            "Epoch [351/2500], Loss: 0.3651\n",
            "Epoch [352/2500], Loss: 0.3654\n",
            "Epoch [353/2500], Loss: 0.3665\n",
            "Epoch [354/2500], Loss: 0.3669\n",
            "Epoch [355/2500], Loss: 0.3681\n",
            "Epoch [356/2500], Loss: 0.3660\n",
            "Epoch [357/2500], Loss: 0.3641\n",
            "Epoch [358/2500], Loss: 0.3620\n",
            "Epoch [359/2500], Loss: 0.3612\n",
            "Epoch [360/2500], Loss: 0.3616\n",
            "Epoch [361/2500], Loss: 0.3622\n",
            "Epoch [362/2500], Loss: 0.3629\n",
            "Epoch [363/2500], Loss: 0.3621\n",
            "Epoch [364/2500], Loss: 0.3610\n",
            "Epoch [365/2500], Loss: 0.3595\n",
            "Epoch [366/2500], Loss: 0.3587\n",
            "Epoch [367/2500], Loss: 0.3587\n",
            "Epoch [368/2500], Loss: 0.3592\n",
            "Epoch [369/2500], Loss: 0.3597\n",
            "Epoch [370/2500], Loss: 0.3595\n",
            "Epoch [371/2500], Loss: 0.3591\n",
            "Epoch [372/2500], Loss: 0.3580\n",
            "Epoch [373/2500], Loss: 0.3573\n",
            "Epoch [374/2500], Loss: 0.3566\n",
            "Epoch [375/2500], Loss: 0.3560\n",
            "Epoch [376/2500], Loss: 0.3556\n",
            "Epoch [377/2500], Loss: 0.3554\n",
            "Epoch [378/2500], Loss: 0.3555\n",
            "Epoch [379/2500], Loss: 0.3556\n",
            "Epoch [380/2500], Loss: 0.3561\n",
            "Epoch [381/2500], Loss: 0.3562\n",
            "Epoch [382/2500], Loss: 0.3570\n",
            "Epoch [383/2500], Loss: 0.3568\n",
            "Epoch [384/2500], Loss: 0.3572\n",
            "Epoch [385/2500], Loss: 0.3560\n",
            "Epoch [386/2500], Loss: 0.3547\n",
            "Epoch [387/2500], Loss: 0.3530\n",
            "Epoch [388/2500], Loss: 0.3519\n",
            "Epoch [389/2500], Loss: 0.3516\n",
            "Epoch [390/2500], Loss: 0.3520\n",
            "Epoch [391/2500], Loss: 0.3527\n",
            "Epoch [392/2500], Loss: 0.3528\n",
            "Epoch [393/2500], Loss: 0.3530\n",
            "Epoch [394/2500], Loss: 0.3522\n",
            "Epoch [395/2500], Loss: 0.3518\n",
            "Epoch [396/2500], Loss: 0.3508\n",
            "Epoch [397/2500], Loss: 0.3499\n",
            "Epoch [398/2500], Loss: 0.3490\n",
            "Epoch [399/2500], Loss: 0.3485\n",
            "Epoch [400/2500], Loss: 0.3484\n",
            "Epoch [401/2500], Loss: 0.3484\n",
            "Epoch [402/2500], Loss: 0.3486\n",
            "Epoch [403/2500], Loss: 0.3486\n",
            "Epoch [404/2500], Loss: 0.3491\n",
            "Epoch [405/2500], Loss: 0.3495\n",
            "Epoch [406/2500], Loss: 0.3508\n",
            "Epoch [407/2500], Loss: 0.3511\n",
            "Epoch [408/2500], Loss: 0.3515\n",
            "Epoch [409/2500], Loss: 0.3492\n",
            "Epoch [410/2500], Loss: 0.3472\n",
            "Epoch [411/2500], Loss: 0.3456\n",
            "Epoch [412/2500], Loss: 0.3452\n",
            "Epoch [413/2500], Loss: 0.3454\n",
            "Epoch [414/2500], Loss: 0.3458\n",
            "Epoch [415/2500], Loss: 0.3464\n",
            "Epoch [416/2500], Loss: 0.3462\n",
            "Epoch [417/2500], Loss: 0.3462\n",
            "Epoch [418/2500], Loss: 0.3448\n",
            "Epoch [419/2500], Loss: 0.3436\n",
            "Epoch [420/2500], Loss: 0.3427\n",
            "Epoch [421/2500], Loss: 0.3425\n",
            "Epoch [422/2500], Loss: 0.3426\n",
            "Epoch [423/2500], Loss: 0.3425\n",
            "Epoch [424/2500], Loss: 0.3426\n",
            "Epoch [425/2500], Loss: 0.3426\n",
            "Epoch [426/2500], Loss: 0.3432\n",
            "Epoch [427/2500], Loss: 0.3435\n",
            "Epoch [428/2500], Loss: 0.3440\n",
            "Epoch [429/2500], Loss: 0.3434\n",
            "Epoch [430/2500], Loss: 0.3431\n",
            "Epoch [431/2500], Loss: 0.3419\n",
            "Epoch [432/2500], Loss: 0.3412\n",
            "Epoch [433/2500], Loss: 0.3400\n",
            "Epoch [434/2500], Loss: 0.3391\n",
            "Epoch [435/2500], Loss: 0.3386\n",
            "Epoch [436/2500], Loss: 0.3385\n",
            "Epoch [437/2500], Loss: 0.3387\n",
            "Epoch [438/2500], Loss: 0.3389\n",
            "Epoch [439/2500], Loss: 0.3393\n",
            "Epoch [440/2500], Loss: 0.3396\n",
            "Epoch [441/2500], Loss: 0.3407\n",
            "Epoch [442/2500], Loss: 0.3411\n",
            "Epoch [443/2500], Loss: 0.3420\n",
            "Epoch [444/2500], Loss: 0.3405\n",
            "Epoch [445/2500], Loss: 0.3392\n",
            "Epoch [446/2500], Loss: 0.3371\n",
            "Epoch [447/2500], Loss: 0.3360\n",
            "Epoch [448/2500], Loss: 0.3355\n",
            "Epoch [449/2500], Loss: 0.3355\n",
            "Epoch [450/2500], Loss: 0.3360\n",
            "Epoch [451/2500], Loss: 0.3365\n",
            "Epoch [452/2500], Loss: 0.3373\n",
            "Epoch [453/2500], Loss: 0.3368\n",
            "Epoch [454/2500], Loss: 0.3363\n",
            "Epoch [455/2500], Loss: 0.3350\n",
            "Epoch [456/2500], Loss: 0.3342\n",
            "Epoch [457/2500], Loss: 0.3335\n",
            "Epoch [458/2500], Loss: 0.3330\n",
            "Epoch [459/2500], Loss: 0.3326\n",
            "Epoch [460/2500], Loss: 0.3323\n",
            "Epoch [461/2500], Loss: 0.3323\n",
            "Epoch [462/2500], Loss: 0.3325\n",
            "Epoch [463/2500], Loss: 0.3329\n",
            "Epoch [464/2500], Loss: 0.3333\n",
            "Epoch [465/2500], Loss: 0.3344\n",
            "Epoch [466/2500], Loss: 0.3350\n",
            "Epoch [467/2500], Loss: 0.3368\n",
            "Epoch [468/2500], Loss: 0.3360\n",
            "Epoch [469/2500], Loss: 0.3358\n",
            "Epoch [470/2500], Loss: 0.3326\n",
            "Epoch [471/2500], Loss: 0.3304\n",
            "Epoch [472/2500], Loss: 0.3293\n",
            "Epoch [473/2500], Loss: 0.3296\n",
            "Epoch [474/2500], Loss: 0.3306\n",
            "Epoch [475/2500], Loss: 0.3312\n",
            "Epoch [476/2500], Loss: 0.3318\n",
            "Epoch [477/2500], Loss: 0.3309\n",
            "Epoch [478/2500], Loss: 0.3301\n",
            "Epoch [479/2500], Loss: 0.3288\n",
            "Epoch [480/2500], Loss: 0.3277\n",
            "Epoch [481/2500], Loss: 0.3271\n",
            "Epoch [482/2500], Loss: 0.3271\n",
            "Epoch [483/2500], Loss: 0.3274\n",
            "Epoch [484/2500], Loss: 0.3278\n",
            "Epoch [485/2500], Loss: 0.3283\n",
            "Epoch [486/2500], Loss: 0.3283\n",
            "Epoch [487/2500], Loss: 0.3286\n",
            "Epoch [488/2500], Loss: 0.3281\n",
            "Epoch [489/2500], Loss: 0.3280\n",
            "Epoch [490/2500], Loss: 0.3271\n",
            "Epoch [491/2500], Loss: 0.3265\n",
            "Epoch [492/2500], Loss: 0.3255\n",
            "Epoch [493/2500], Loss: 0.3248\n",
            "Epoch [494/2500], Loss: 0.3242\n",
            "Epoch [495/2500], Loss: 0.3239\n",
            "Epoch [496/2500], Loss: 0.3237\n",
            "Epoch [497/2500], Loss: 0.3235\n",
            "Epoch [498/2500], Loss: 0.3234\n",
            "Epoch [499/2500], Loss: 0.3232\n",
            "Epoch [500/2500], Loss: 0.3233\n",
            "Epoch [501/2500], Loss: 0.3235\n",
            "Epoch [502/2500], Loss: 0.3245\n",
            "Epoch [503/2500], Loss: 0.3261\n",
            "Epoch [504/2500], Loss: 0.3299\n",
            "Epoch [505/2500], Loss: 0.3316\n",
            "Epoch [506/2500], Loss: 0.3348\n",
            "Epoch [507/2500], Loss: 0.3281\n",
            "Epoch [508/2500], Loss: 0.3233\n",
            "Epoch [509/2500], Loss: 0.3211\n",
            "Epoch [510/2500], Loss: 0.3226\n",
            "Epoch [511/2500], Loss: 0.3253\n",
            "Epoch [512/2500], Loss: 0.3247\n",
            "Epoch [513/2500], Loss: 0.3235\n",
            "Epoch [514/2500], Loss: 0.3211\n",
            "Epoch [515/2500], Loss: 0.3204\n",
            "Epoch [516/2500], Loss: 0.3206\n",
            "Epoch [517/2500], Loss: 0.3212\n",
            "Epoch [518/2500], Loss: 0.3221\n",
            "Epoch [519/2500], Loss: 0.3211\n",
            "Epoch [520/2500], Loss: 0.3200\n",
            "Epoch [521/2500], Loss: 0.3184\n",
            "Epoch [522/2500], Loss: 0.3179\n",
            "Epoch [523/2500], Loss: 0.3187\n",
            "Epoch [524/2500], Loss: 0.3193\n",
            "Epoch [525/2500], Loss: 0.3196\n",
            "Epoch [526/2500], Loss: 0.3185\n",
            "Epoch [527/2500], Loss: 0.3178\n",
            "Epoch [528/2500], Loss: 0.3173\n",
            "Epoch [529/2500], Loss: 0.3169\n",
            "Epoch [530/2500], Loss: 0.3164\n",
            "Epoch [531/2500], Loss: 0.3160\n",
            "Epoch [532/2500], Loss: 0.3159\n",
            "Epoch [533/2500], Loss: 0.3161\n",
            "Epoch [534/2500], Loss: 0.3164\n",
            "Epoch [535/2500], Loss: 0.3165\n",
            "Epoch [536/2500], Loss: 0.3165\n",
            "Epoch [537/2500], Loss: 0.3164\n",
            "Epoch [538/2500], Loss: 0.3169\n",
            "Epoch [539/2500], Loss: 0.3169\n",
            "Epoch [540/2500], Loss: 0.3175\n",
            "Epoch [541/2500], Loss: 0.3168\n",
            "Epoch [542/2500], Loss: 0.3168\n",
            "Epoch [543/2500], Loss: 0.3158\n",
            "Epoch [544/2500], Loss: 0.3153\n",
            "Epoch [545/2500], Loss: 0.3145\n",
            "Epoch [546/2500], Loss: 0.3137\n",
            "Epoch [547/2500], Loss: 0.3129\n",
            "Epoch [548/2500], Loss: 0.3124\n",
            "Epoch [549/2500], Loss: 0.3121\n",
            "Epoch [550/2500], Loss: 0.3119\n",
            "Epoch [551/2500], Loss: 0.3118\n",
            "Epoch [552/2500], Loss: 0.3117\n",
            "Epoch [553/2500], Loss: 0.3116\n",
            "Epoch [554/2500], Loss: 0.3117\n",
            "Epoch [555/2500], Loss: 0.3123\n",
            "Epoch [556/2500], Loss: 0.3133\n",
            "Epoch [557/2500], Loss: 0.3160\n",
            "Epoch [558/2500], Loss: 0.3184\n",
            "Epoch [559/2500], Loss: 0.3246\n",
            "Epoch [560/2500], Loss: 0.3212\n",
            "Epoch [561/2500], Loss: 0.3187\n",
            "Epoch [562/2500], Loss: 0.3119\n",
            "Epoch [563/2500], Loss: 0.3094\n",
            "Epoch [564/2500], Loss: 0.3113\n",
            "Epoch [565/2500], Loss: 0.3142\n",
            "Epoch [566/2500], Loss: 0.3161\n",
            "Epoch [567/2500], Loss: 0.3121\n",
            "Epoch [568/2500], Loss: 0.3089\n",
            "Epoch [569/2500], Loss: 0.3082\n",
            "Epoch [570/2500], Loss: 0.3098\n",
            "Epoch [571/2500], Loss: 0.3121\n",
            "Epoch [572/2500], Loss: 0.3114\n",
            "Epoch [573/2500], Loss: 0.3099\n",
            "Epoch [574/2500], Loss: 0.3076\n",
            "Epoch [575/2500], Loss: 0.3069\n",
            "Epoch [576/2500], Loss: 0.3075\n",
            "Epoch [577/2500], Loss: 0.3085\n",
            "Epoch [578/2500], Loss: 0.3092\n",
            "Epoch [579/2500], Loss: 0.3084\n",
            "Epoch [580/2500], Loss: 0.3074\n",
            "Epoch [581/2500], Loss: 0.3061\n",
            "Epoch [582/2500], Loss: 0.3055\n",
            "Epoch [583/2500], Loss: 0.3054\n",
            "Epoch [584/2500], Loss: 0.3057\n",
            "Epoch [585/2500], Loss: 0.3062\n",
            "Epoch [586/2500], Loss: 0.3064\n",
            "Epoch [587/2500], Loss: 0.3065\n",
            "Epoch [588/2500], Loss: 0.3060\n",
            "Epoch [589/2500], Loss: 0.3055\n",
            "Epoch [590/2500], Loss: 0.3048\n",
            "Epoch [591/2500], Loss: 0.3042\n",
            "Epoch [592/2500], Loss: 0.3037\n",
            "Epoch [593/2500], Loss: 0.3033\n",
            "Epoch [594/2500], Loss: 0.3030\n",
            "Epoch [595/2500], Loss: 0.3028\n",
            "Epoch [596/2500], Loss: 0.3027\n",
            "Epoch [597/2500], Loss: 0.3027\n",
            "Epoch [598/2500], Loss: 0.3027\n",
            "Epoch [599/2500], Loss: 0.3029\n",
            "Epoch [600/2500], Loss: 0.3035\n",
            "Epoch [601/2500], Loss: 0.3044\n",
            "Epoch [602/2500], Loss: 0.3067\n",
            "Epoch [603/2500], Loss: 0.3085\n",
            "Epoch [604/2500], Loss: 0.3130\n",
            "Epoch [605/2500], Loss: 0.3113\n",
            "Epoch [606/2500], Loss: 0.3104\n",
            "Epoch [607/2500], Loss: 0.3043\n",
            "Epoch [608/2500], Loss: 0.3008\n",
            "Epoch [609/2500], Loss: 0.3006\n",
            "Epoch [610/2500], Loss: 0.3028\n",
            "Epoch [611/2500], Loss: 0.3057\n",
            "Epoch [612/2500], Loss: 0.3048\n",
            "Epoch [613/2500], Loss: 0.3032\n",
            "Epoch [614/2500], Loss: 0.3004\n",
            "Epoch [615/2500], Loss: 0.2992\n",
            "Epoch [616/2500], Loss: 0.2997\n",
            "Epoch [617/2500], Loss: 0.3009\n",
            "Epoch [618/2500], Loss: 0.3022\n",
            "Epoch [619/2500], Loss: 0.3017\n",
            "Epoch [620/2500], Loss: 0.3008\n",
            "Epoch [621/2500], Loss: 0.2991\n",
            "Epoch [622/2500], Loss: 0.2980\n",
            "Epoch [623/2500], Loss: 0.2978\n",
            "Epoch [624/2500], Loss: 0.2982\n",
            "Epoch [625/2500], Loss: 0.2989\n",
            "Epoch [626/2500], Loss: 0.2992\n",
            "Epoch [627/2500], Loss: 0.2993\n",
            "Epoch [628/2500], Loss: 0.2985\n",
            "Epoch [629/2500], Loss: 0.2977\n",
            "Epoch [630/2500], Loss: 0.2970\n",
            "Epoch [631/2500], Loss: 0.2967\n",
            "Epoch [632/2500], Loss: 0.2967\n",
            "Epoch [633/2500], Loss: 0.2970\n",
            "Epoch [634/2500], Loss: 0.2972\n",
            "Epoch [635/2500], Loss: 0.2973\n",
            "Epoch [636/2500], Loss: 0.2967\n",
            "Epoch [637/2500], Loss: 0.2960\n",
            "Epoch [638/2500], Loss: 0.2954\n",
            "Epoch [639/2500], Loss: 0.2953\n",
            "Epoch [640/2500], Loss: 0.2957\n",
            "Epoch [641/2500], Loss: 0.2966\n",
            "Epoch [642/2500], Loss: 0.2975\n",
            "Epoch [643/2500], Loss: 0.2991\n",
            "Epoch [644/2500], Loss: 0.2998\n",
            "Epoch [645/2500], Loss: 0.3024\n",
            "Epoch [646/2500], Loss: 0.3017\n",
            "Epoch [647/2500], Loss: 0.3033\n",
            "Epoch [648/2500], Loss: 0.2984\n",
            "Epoch [649/2500], Loss: 0.2951\n",
            "Epoch [650/2500], Loss: 0.2928\n",
            "Epoch [651/2500], Loss: 0.2932\n",
            "Epoch [652/2500], Loss: 0.2954\n",
            "Epoch [653/2500], Loss: 0.2963\n",
            "Epoch [654/2500], Loss: 0.2971\n",
            "Epoch [655/2500], Loss: 0.2954\n",
            "Epoch [656/2500], Loss: 0.2947\n",
            "Epoch [657/2500], Loss: 0.2937\n",
            "Epoch [658/2500], Loss: 0.2926\n",
            "Epoch [659/2500], Loss: 0.2914\n",
            "Epoch [660/2500], Loss: 0.2911\n",
            "Epoch [661/2500], Loss: 0.2917\n",
            "Epoch [662/2500], Loss: 0.2925\n",
            "Epoch [663/2500], Loss: 0.2930\n",
            "Epoch [664/2500], Loss: 0.2927\n",
            "Epoch [665/2500], Loss: 0.2926\n",
            "Epoch [666/2500], Loss: 0.2923\n",
            "Epoch [667/2500], Loss: 0.2925\n",
            "Epoch [668/2500], Loss: 0.2917\n",
            "Epoch [669/2500], Loss: 0.2910\n",
            "Epoch [670/2500], Loss: 0.2901\n",
            "Epoch [671/2500], Loss: 0.2897\n",
            "Epoch [672/2500], Loss: 0.2895\n",
            "Epoch [673/2500], Loss: 0.2892\n",
            "Epoch [674/2500], Loss: 0.2888\n",
            "Epoch [675/2500], Loss: 0.2883\n",
            "Epoch [676/2500], Loss: 0.2881\n",
            "Epoch [677/2500], Loss: 0.2880\n",
            "Epoch [678/2500], Loss: 0.2879\n",
            "Epoch [679/2500], Loss: 0.2878\n",
            "Epoch [680/2500], Loss: 0.2875\n",
            "Epoch [681/2500], Loss: 0.2872\n",
            "Epoch [682/2500], Loss: 0.2870\n",
            "Epoch [683/2500], Loss: 0.2869\n",
            "Epoch [684/2500], Loss: 0.2868\n",
            "Epoch [685/2500], Loss: 0.2867\n",
            "Epoch [686/2500], Loss: 0.2865\n",
            "Epoch [687/2500], Loss: 0.2864\n",
            "Epoch [688/2500], Loss: 0.2865\n",
            "Epoch [689/2500], Loss: 0.2871\n",
            "Epoch [690/2500], Loss: 0.2892\n",
            "Epoch [691/2500], Loss: 0.2938\n",
            "Epoch [692/2500], Loss: 0.3073\n",
            "Epoch [693/2500], Loss: 0.3127\n",
            "Epoch [694/2500], Loss: 0.3227\n",
            "Epoch [695/2500], Loss: 0.2931\n",
            "Epoch [696/2500], Loss: 0.2868\n",
            "Epoch [697/2500], Loss: 0.2997\n",
            "Epoch [698/2500], Loss: 0.2993\n",
            "Epoch [699/2500], Loss: 0.2919\n",
            "Epoch [700/2500], Loss: 0.2870\n",
            "Epoch [701/2500], Loss: 0.2923\n",
            "Epoch [702/2500], Loss: 0.2958\n",
            "Epoch [703/2500], Loss: 0.2882\n",
            "Epoch [704/2500], Loss: 0.2871\n",
            "Epoch [705/2500], Loss: 0.2896\n",
            "Epoch [706/2500], Loss: 0.2892\n",
            "Epoch [707/2500], Loss: 0.2888\n",
            "Epoch [708/2500], Loss: 0.2854\n",
            "Epoch [709/2500], Loss: 0.2853\n",
            "Epoch [710/2500], Loss: 0.2882\n",
            "Epoch [711/2500], Loss: 0.2866\n",
            "Epoch [712/2500], Loss: 0.2838\n",
            "Epoch [713/2500], Loss: 0.2828\n",
            "Epoch [714/2500], Loss: 0.2850\n",
            "Epoch [715/2500], Loss: 0.2865\n",
            "Epoch [716/2500], Loss: 0.2834\n",
            "Epoch [717/2500], Loss: 0.2821\n",
            "Epoch [718/2500], Loss: 0.2832\n",
            "Epoch [719/2500], Loss: 0.2834\n",
            "Epoch [720/2500], Loss: 0.2829\n",
            "Epoch [721/2500], Loss: 0.2823\n",
            "Epoch [722/2500], Loss: 0.2820\n",
            "Epoch [723/2500], Loss: 0.2813\n",
            "Epoch [724/2500], Loss: 0.2811\n",
            "Epoch [725/2500], Loss: 0.2817\n",
            "Epoch [726/2500], Loss: 0.2818\n",
            "Epoch [727/2500], Loss: 0.2809\n",
            "Epoch [728/2500], Loss: 0.2801\n",
            "Epoch [729/2500], Loss: 0.2801\n",
            "Epoch [730/2500], Loss: 0.2802\n",
            "Epoch [731/2500], Loss: 0.2801\n",
            "Epoch [732/2500], Loss: 0.2799\n",
            "Epoch [733/2500], Loss: 0.2799\n",
            "Epoch [734/2500], Loss: 0.2797\n",
            "Epoch [735/2500], Loss: 0.2792\n",
            "Epoch [736/2500], Loss: 0.2787\n",
            "Epoch [737/2500], Loss: 0.2786\n",
            "Epoch [738/2500], Loss: 0.2787\n",
            "Epoch [739/2500], Loss: 0.2786\n",
            "Epoch [740/2500], Loss: 0.2784\n",
            "Epoch [741/2500], Loss: 0.2783\n",
            "Epoch [742/2500], Loss: 0.2783\n",
            "Epoch [743/2500], Loss: 0.2782\n",
            "Epoch [744/2500], Loss: 0.2780\n",
            "Epoch [745/2500], Loss: 0.2778\n",
            "Epoch [746/2500], Loss: 0.2777\n",
            "Epoch [747/2500], Loss: 0.2776\n",
            "Epoch [748/2500], Loss: 0.2775\n",
            "Epoch [749/2500], Loss: 0.2775\n",
            "Epoch [750/2500], Loss: 0.2774\n",
            "Epoch [751/2500], Loss: 0.2777\n",
            "Epoch [752/2500], Loss: 0.2781\n",
            "Epoch [753/2500], Loss: 0.2789\n",
            "Epoch [754/2500], Loss: 0.2797\n",
            "Epoch [755/2500], Loss: 0.2818\n",
            "Epoch [756/2500], Loss: 0.2828\n",
            "Epoch [757/2500], Loss: 0.2855\n",
            "Epoch [758/2500], Loss: 0.2834\n",
            "Epoch [759/2500], Loss: 0.2824\n",
            "Epoch [760/2500], Loss: 0.2784\n",
            "Epoch [761/2500], Loss: 0.2759\n",
            "Epoch [762/2500], Loss: 0.2748\n",
            "Epoch [763/2500], Loss: 0.2753\n",
            "Epoch [764/2500], Loss: 0.2768\n",
            "Epoch [765/2500], Loss: 0.2781\n",
            "Epoch [766/2500], Loss: 0.2795\n",
            "Epoch [767/2500], Loss: 0.2786\n",
            "Epoch [768/2500], Loss: 0.2778\n",
            "Epoch [769/2500], Loss: 0.2758\n",
            "Epoch [770/2500], Loss: 0.2744\n",
            "Epoch [771/2500], Loss: 0.2735\n",
            "Epoch [772/2500], Loss: 0.2733\n",
            "Epoch [773/2500], Loss: 0.2736\n",
            "Epoch [774/2500], Loss: 0.2742\n",
            "Epoch [775/2500], Loss: 0.2750\n",
            "Epoch [776/2500], Loss: 0.2755\n",
            "Epoch [777/2500], Loss: 0.2764\n",
            "Epoch [778/2500], Loss: 0.2762\n",
            "Epoch [779/2500], Loss: 0.2766\n",
            "Epoch [780/2500], Loss: 0.2756\n",
            "Epoch [781/2500], Loss: 0.2750\n",
            "Epoch [782/2500], Loss: 0.2737\n",
            "Epoch [783/2500], Loss: 0.2728\n",
            "Epoch [784/2500], Loss: 0.2719\n",
            "Epoch [785/2500], Loss: 0.2714\n",
            "Epoch [786/2500], Loss: 0.2712\n",
            "Epoch [787/2500], Loss: 0.2712\n",
            "Epoch [788/2500], Loss: 0.2713\n",
            "Epoch [789/2500], Loss: 0.2716\n",
            "Epoch [790/2500], Loss: 0.2721\n",
            "Epoch [791/2500], Loss: 0.2728\n",
            "Epoch [792/2500], Loss: 0.2743\n",
            "Epoch [793/2500], Loss: 0.2755\n",
            "Epoch [794/2500], Loss: 0.2782\n",
            "Epoch [795/2500], Loss: 0.2783\n",
            "Epoch [796/2500], Loss: 0.2797\n",
            "Epoch [797/2500], Loss: 0.2763\n",
            "Epoch [798/2500], Loss: 0.2738\n",
            "Epoch [799/2500], Loss: 0.2708\n",
            "Epoch [800/2500], Loss: 0.2694\n",
            "Epoch [801/2500], Loss: 0.2696\n",
            "Epoch [802/2500], Loss: 0.2707\n",
            "Epoch [803/2500], Loss: 0.2724\n",
            "Epoch [804/2500], Loss: 0.2731\n",
            "Epoch [805/2500], Loss: 0.2740\n",
            "Epoch [806/2500], Loss: 0.2725\n",
            "Epoch [807/2500], Loss: 0.2715\n",
            "Epoch [808/2500], Loss: 0.2697\n",
            "Epoch [809/2500], Loss: 0.2685\n",
            "Epoch [810/2500], Loss: 0.2679\n",
            "Epoch [811/2500], Loss: 0.2678\n",
            "Epoch [812/2500], Loss: 0.2682\n",
            "Epoch [813/2500], Loss: 0.2687\n",
            "Epoch [814/2500], Loss: 0.2695\n",
            "Epoch [815/2500], Loss: 0.2701\n",
            "Epoch [816/2500], Loss: 0.2712\n",
            "Epoch [817/2500], Loss: 0.2714\n",
            "Epoch [818/2500], Loss: 0.2722\n",
            "Epoch [819/2500], Loss: 0.2713\n",
            "Epoch [820/2500], Loss: 0.2707\n",
            "Epoch [821/2500], Loss: 0.2689\n",
            "Epoch [822/2500], Loss: 0.2676\n",
            "Epoch [823/2500], Loss: 0.2665\n",
            "Epoch [824/2500], Loss: 0.2660\n",
            "Epoch [825/2500], Loss: 0.2659\n",
            "Epoch [826/2500], Loss: 0.2661\n",
            "Epoch [827/2500], Loss: 0.2665\n",
            "Epoch [828/2500], Loss: 0.2670\n",
            "Epoch [829/2500], Loss: 0.2679\n",
            "Epoch [830/2500], Loss: 0.2686\n",
            "Epoch [831/2500], Loss: 0.2702\n",
            "Epoch [832/2500], Loss: 0.2705\n",
            "Epoch [833/2500], Loss: 0.2719\n",
            "Epoch [834/2500], Loss: 0.2705\n",
            "Epoch [835/2500], Loss: 0.2700\n",
            "Epoch [836/2500], Loss: 0.2675\n",
            "Epoch [837/2500], Loss: 0.2658\n",
            "Epoch [838/2500], Loss: 0.2645\n",
            "Epoch [839/2500], Loss: 0.2639\n",
            "Epoch [840/2500], Loss: 0.2640\n",
            "Epoch [841/2500], Loss: 0.2645\n",
            "Epoch [842/2500], Loss: 0.2653\n",
            "Epoch [843/2500], Loss: 0.2661\n",
            "Epoch [844/2500], Loss: 0.2673\n",
            "Epoch [845/2500], Loss: 0.2676\n",
            "Epoch [846/2500], Loss: 0.2687\n",
            "Epoch [847/2500], Loss: 0.2680\n",
            "Epoch [848/2500], Loss: 0.2679\n",
            "Epoch [849/2500], Loss: 0.2663\n",
            "Epoch [850/2500], Loss: 0.2651\n",
            "Epoch [851/2500], Loss: 0.2637\n",
            "Epoch [852/2500], Loss: 0.2627\n",
            "Epoch [853/2500], Loss: 0.2621\n",
            "Epoch [854/2500], Loss: 0.2620\n",
            "Epoch [855/2500], Loss: 0.2623\n",
            "Epoch [856/2500], Loss: 0.2627\n",
            "Epoch [857/2500], Loss: 0.2635\n",
            "Epoch [858/2500], Loss: 0.2642\n",
            "Epoch [859/2500], Loss: 0.2655\n",
            "Epoch [860/2500], Loss: 0.2660\n",
            "Epoch [861/2500], Loss: 0.2673\n",
            "Epoch [862/2500], Loss: 0.2667\n",
            "Epoch [863/2500], Loss: 0.2668\n",
            "Epoch [864/2500], Loss: 0.2648\n",
            "Epoch [865/2500], Loss: 0.2635\n",
            "Epoch [866/2500], Loss: 0.2618\n",
            "Epoch [867/2500], Loss: 0.2610\n",
            "Epoch [868/2500], Loss: 0.2607\n",
            "Epoch [869/2500], Loss: 0.2608\n",
            "Epoch [870/2500], Loss: 0.2611\n",
            "Epoch [871/2500], Loss: 0.2613\n",
            "Epoch [872/2500], Loss: 0.2617\n",
            "Epoch [873/2500], Loss: 0.2620\n",
            "Epoch [874/2500], Loss: 0.2626\n",
            "Epoch [875/2500], Loss: 0.2628\n",
            "Epoch [876/2500], Loss: 0.2639\n",
            "Epoch [877/2500], Loss: 0.2637\n",
            "Epoch [878/2500], Loss: 0.2647\n",
            "Epoch [879/2500], Loss: 0.2634\n",
            "Epoch [880/2500], Loss: 0.2631\n",
            "Epoch [881/2500], Loss: 0.2612\n",
            "Epoch [882/2500], Loss: 0.2600\n",
            "Epoch [883/2500], Loss: 0.2588\n",
            "Epoch [884/2500], Loss: 0.2582\n",
            "Epoch [885/2500], Loss: 0.2580\n",
            "Epoch [886/2500], Loss: 0.2582\n",
            "Epoch [887/2500], Loss: 0.2585\n",
            "Epoch [888/2500], Loss: 0.2587\n",
            "Epoch [889/2500], Loss: 0.2590\n",
            "Epoch [890/2500], Loss: 0.2589\n",
            "Epoch [891/2500], Loss: 0.2593\n",
            "Epoch [892/2500], Loss: 0.2596\n",
            "Epoch [893/2500], Loss: 0.2610\n",
            "Epoch [894/2500], Loss: 0.2624\n",
            "Epoch [895/2500], Loss: 0.2656\n",
            "Epoch [896/2500], Loss: 0.2668\n",
            "Epoch [897/2500], Loss: 0.2684\n",
            "Epoch [898/2500], Loss: 0.2642\n",
            "Epoch [899/2500], Loss: 0.2604\n",
            "Epoch [900/2500], Loss: 0.2573\n",
            "Epoch [901/2500], Loss: 0.2571\n",
            "Epoch [902/2500], Loss: 0.2586\n",
            "Epoch [903/2500], Loss: 0.2595\n",
            "Epoch [904/2500], Loss: 0.2594\n",
            "Epoch [905/2500], Loss: 0.2584\n",
            "Epoch [906/2500], Loss: 0.2583\n",
            "Epoch [907/2500], Loss: 0.2582\n",
            "Epoch [908/2500], Loss: 0.2585\n",
            "Epoch [909/2500], Loss: 0.2571\n",
            "Epoch [910/2500], Loss: 0.2557\n",
            "Epoch [911/2500], Loss: 0.2548\n",
            "Epoch [912/2500], Loss: 0.2549\n",
            "Epoch [913/2500], Loss: 0.2557\n",
            "Epoch [914/2500], Loss: 0.2560\n",
            "Epoch [915/2500], Loss: 0.2560\n",
            "Epoch [916/2500], Loss: 0.2554\n",
            "Epoch [917/2500], Loss: 0.2555\n",
            "Epoch [918/2500], Loss: 0.2560\n",
            "Epoch [919/2500], Loss: 0.2571\n",
            "Epoch [920/2500], Loss: 0.2578\n",
            "Epoch [921/2500], Loss: 0.2587\n",
            "Epoch [922/2500], Loss: 0.2584\n",
            "Epoch [923/2500], Loss: 0.2590\n",
            "Epoch [924/2500], Loss: 0.2583\n",
            "Epoch [925/2500], Loss: 0.2589\n",
            "Epoch [926/2500], Loss: 0.2570\n",
            "Epoch [927/2500], Loss: 0.2559\n",
            "Epoch [928/2500], Loss: 0.2540\n",
            "Epoch [929/2500], Loss: 0.2531\n",
            "Epoch [930/2500], Loss: 0.2528\n",
            "Epoch [931/2500], Loss: 0.2529\n",
            "Epoch [932/2500], Loss: 0.2529\n",
            "Epoch [933/2500], Loss: 0.2528\n",
            "Epoch [934/2500], Loss: 0.2529\n",
            "Epoch [935/2500], Loss: 0.2533\n",
            "Epoch [936/2500], Loss: 0.2542\n",
            "Epoch [937/2500], Loss: 0.2551\n",
            "Epoch [938/2500], Loss: 0.2565\n",
            "Epoch [939/2500], Loss: 0.2572\n",
            "Epoch [940/2500], Loss: 0.2589\n",
            "Epoch [941/2500], Loss: 0.2584\n",
            "Epoch [942/2500], Loss: 0.2594\n",
            "Epoch [943/2500], Loss: 0.2567\n",
            "Epoch [944/2500], Loss: 0.2552\n",
            "Epoch [945/2500], Loss: 0.2525\n",
            "Epoch [946/2500], Loss: 0.2510\n",
            "Epoch [947/2500], Loss: 0.2506\n",
            "Epoch [948/2500], Loss: 0.2510\n",
            "Epoch [949/2500], Loss: 0.2520\n",
            "Epoch [950/2500], Loss: 0.2528\n",
            "Epoch [951/2500], Loss: 0.2540\n",
            "Epoch [952/2500], Loss: 0.2543\n",
            "Epoch [953/2500], Loss: 0.2553\n",
            "Epoch [954/2500], Loss: 0.2549\n",
            "Epoch [955/2500], Loss: 0.2546\n",
            "Epoch [956/2500], Loss: 0.2529\n",
            "Epoch [957/2500], Loss: 0.2514\n",
            "Epoch [958/2500], Loss: 0.2501\n",
            "Epoch [959/2500], Loss: 0.2495\n",
            "Epoch [960/2500], Loss: 0.2493\n",
            "Epoch [961/2500], Loss: 0.2494\n",
            "Epoch [962/2500], Loss: 0.2496\n",
            "Epoch [963/2500], Loss: 0.2499\n",
            "Epoch [964/2500], Loss: 0.2506\n",
            "Epoch [965/2500], Loss: 0.2511\n",
            "Epoch [966/2500], Loss: 0.2523\n",
            "Epoch [967/2500], Loss: 0.2526\n",
            "Epoch [968/2500], Loss: 0.2538\n",
            "Epoch [969/2500], Loss: 0.2532\n",
            "Epoch [970/2500], Loss: 0.2535\n",
            "Epoch [971/2500], Loss: 0.2522\n",
            "Epoch [972/2500], Loss: 0.2513\n",
            "Epoch [973/2500], Loss: 0.2498\n",
            "Epoch [974/2500], Loss: 0.2487\n",
            "Epoch [975/2500], Loss: 0.2478\n",
            "Epoch [976/2500], Loss: 0.2473\n",
            "Epoch [977/2500], Loss: 0.2472\n",
            "Epoch [978/2500], Loss: 0.2473\n",
            "Epoch [979/2500], Loss: 0.2476\n",
            "Epoch [980/2500], Loss: 0.2480\n",
            "Epoch [981/2500], Loss: 0.2487\n",
            "Epoch [982/2500], Loss: 0.2494\n",
            "Epoch [983/2500], Loss: 0.2507\n",
            "Epoch [984/2500], Loss: 0.2515\n",
            "Epoch [985/2500], Loss: 0.2536\n",
            "Epoch [986/2500], Loss: 0.2535\n",
            "Epoch [987/2500], Loss: 0.2547\n",
            "Epoch [988/2500], Loss: 0.2523\n",
            "Epoch [989/2500], Loss: 0.2508\n",
            "Epoch [990/2500], Loss: 0.2481\n",
            "Epoch [991/2500], Loss: 0.2464\n",
            "Epoch [992/2500], Loss: 0.2456\n",
            "Epoch [993/2500], Loss: 0.2457\n",
            "Epoch [994/2500], Loss: 0.2465\n",
            "Epoch [995/2500], Loss: 0.2474\n",
            "Epoch [996/2500], Loss: 0.2488\n",
            "Epoch [997/2500], Loss: 0.2492\n",
            "Epoch [998/2500], Loss: 0.2502\n",
            "Epoch [999/2500], Loss: 0.2495\n",
            "Epoch [1000/2500], Loss: 0.2492\n",
            "Epoch [1001/2500], Loss: 0.2477\n",
            "Epoch [1002/2500], Loss: 0.2465\n",
            "Epoch [1003/2500], Loss: 0.2453\n",
            "Epoch [1004/2500], Loss: 0.2445\n",
            "Epoch [1005/2500], Loss: 0.2441\n",
            "Epoch [1006/2500], Loss: 0.2441\n",
            "Epoch [1007/2500], Loss: 0.2444\n",
            "Epoch [1008/2500], Loss: 0.2448\n",
            "Epoch [1009/2500], Loss: 0.2454\n",
            "Epoch [1010/2500], Loss: 0.2461\n",
            "Epoch [1011/2500], Loss: 0.2472\n",
            "Epoch [1012/2500], Loss: 0.2478\n",
            "Epoch [1013/2500], Loss: 0.2492\n",
            "Epoch [1014/2500], Loss: 0.2491\n",
            "Epoch [1015/2500], Loss: 0.2497\n",
            "Epoch [1016/2500], Loss: 0.2481\n",
            "Epoch [1017/2500], Loss: 0.2473\n",
            "Epoch [1018/2500], Loss: 0.2453\n",
            "Epoch [1019/2500], Loss: 0.2440\n",
            "Epoch [1020/2500], Loss: 0.2430\n",
            "Epoch [1021/2500], Loss: 0.2425\n",
            "Epoch [1022/2500], Loss: 0.2425\n",
            "Epoch [1023/2500], Loss: 0.2428\n",
            "Epoch [1024/2500], Loss: 0.2434\n",
            "Epoch [1025/2500], Loss: 0.2441\n",
            "Epoch [1026/2500], Loss: 0.2452\n",
            "Epoch [1027/2500], Loss: 0.2458\n",
            "Epoch [1028/2500], Loss: 0.2472\n",
            "Epoch [1029/2500], Loss: 0.2472\n",
            "Epoch [1030/2500], Loss: 0.2480\n",
            "Epoch [1031/2500], Loss: 0.2467\n",
            "Epoch [1032/2500], Loss: 0.2461\n",
            "Epoch [1033/2500], Loss: 0.2441\n",
            "Epoch [1034/2500], Loss: 0.2428\n",
            "Epoch [1035/2500], Loss: 0.2416\n",
            "Epoch [1036/2500], Loss: 0.2410\n",
            "Epoch [1037/2500], Loss: 0.2408\n",
            "Epoch [1038/2500], Loss: 0.2410\n",
            "Epoch [1039/2500], Loss: 0.2415\n",
            "Epoch [1040/2500], Loss: 0.2420\n",
            "Epoch [1041/2500], Loss: 0.2429\n",
            "Epoch [1042/2500], Loss: 0.2436\n",
            "Epoch [1043/2500], Loss: 0.2450\n",
            "Epoch [1044/2500], Loss: 0.2455\n",
            "Epoch [1045/2500], Loss: 0.2467\n",
            "Epoch [1046/2500], Loss: 0.2460\n",
            "Epoch [1047/2500], Loss: 0.2458\n",
            "Epoch [1048/2500], Loss: 0.2437\n",
            "Epoch [1049/2500], Loss: 0.2421\n",
            "Epoch [1050/2500], Loss: 0.2405\n",
            "Epoch [1051/2500], Loss: 0.2396\n",
            "Epoch [1052/2500], Loss: 0.2393\n",
            "Epoch [1053/2500], Loss: 0.2396\n",
            "Epoch [1054/2500], Loss: 0.2402\n",
            "Epoch [1055/2500], Loss: 0.2409\n",
            "Epoch [1056/2500], Loss: 0.2418\n",
            "Epoch [1057/2500], Loss: 0.2423\n",
            "Epoch [1058/2500], Loss: 0.2432\n",
            "Epoch [1059/2500], Loss: 0.2431\n",
            "Epoch [1060/2500], Loss: 0.2435\n",
            "Epoch [1061/2500], Loss: 0.2425\n",
            "Epoch [1062/2500], Loss: 0.2422\n",
            "Epoch [1063/2500], Loss: 0.2410\n",
            "Epoch [1064/2500], Loss: 0.2407\n",
            "Epoch [1065/2500], Loss: 0.2400\n",
            "Epoch [1066/2500], Loss: 0.2400\n",
            "Epoch [1067/2500], Loss: 0.2396\n",
            "Epoch [1068/2500], Loss: 0.2395\n",
            "Epoch [1069/2500], Loss: 0.2390\n",
            "Epoch [1070/2500], Loss: 0.2386\n",
            "Epoch [1071/2500], Loss: 0.2383\n",
            "Epoch [1072/2500], Loss: 0.2381\n",
            "Epoch [1073/2500], Loss: 0.2383\n",
            "Epoch [1074/2500], Loss: 0.2387\n",
            "Epoch [1075/2500], Loss: 0.2398\n",
            "Epoch [1076/2500], Loss: 0.2405\n",
            "Epoch [1077/2500], Loss: 0.2424\n",
            "Epoch [1078/2500], Loss: 0.2427\n",
            "Epoch [1079/2500], Loss: 0.2448\n",
            "Epoch [1080/2500], Loss: 0.2438\n",
            "Epoch [1081/2500], Loss: 0.2444\n",
            "Epoch [1082/2500], Loss: 0.2424\n",
            "Epoch [1083/2500], Loss: 0.2411\n",
            "Epoch [1084/2500], Loss: 0.2392\n",
            "Epoch [1085/2500], Loss: 0.2379\n",
            "Epoch [1086/2500], Loss: 0.2369\n",
            "Epoch [1087/2500], Loss: 0.2366\n",
            "Epoch [1088/2500], Loss: 0.2369\n",
            "Epoch [1089/2500], Loss: 0.2375\n",
            "Epoch [1090/2500], Loss: 0.2386\n",
            "Epoch [1091/2500], Loss: 0.2393\n",
            "Epoch [1092/2500], Loss: 0.2403\n",
            "Epoch [1093/2500], Loss: 0.2401\n",
            "Epoch [1094/2500], Loss: 0.2397\n",
            "Epoch [1095/2500], Loss: 0.2382\n",
            "Epoch [1096/2500], Loss: 0.2370\n",
            "Epoch [1097/2500], Loss: 0.2358\n",
            "Epoch [1098/2500], Loss: 0.2352\n",
            "Epoch [1099/2500], Loss: 0.2351\n",
            "Epoch [1100/2500], Loss: 0.2353\n",
            "Epoch [1101/2500], Loss: 0.2354\n",
            "Epoch [1102/2500], Loss: 0.2355\n",
            "Epoch [1103/2500], Loss: 0.2357\n",
            "Epoch [1104/2500], Loss: 0.2359\n",
            "Epoch [1105/2500], Loss: 0.2364\n",
            "Epoch [1106/2500], Loss: 0.2369\n",
            "Epoch [1107/2500], Loss: 0.2382\n",
            "Epoch [1108/2500], Loss: 0.2387\n",
            "Epoch [1109/2500], Loss: 0.2403\n",
            "Epoch [1110/2500], Loss: 0.2397\n",
            "Epoch [1111/2500], Loss: 0.2403\n",
            "Epoch [1112/2500], Loss: 0.2383\n",
            "Epoch [1113/2500], Loss: 0.2373\n",
            "Epoch [1114/2500], Loss: 0.2356\n",
            "Epoch [1115/2500], Loss: 0.2346\n",
            "Epoch [1116/2500], Loss: 0.2340\n",
            "Epoch [1117/2500], Loss: 0.2337\n",
            "Epoch [1118/2500], Loss: 0.2335\n",
            "Epoch [1119/2500], Loss: 0.2334\n",
            "Epoch [1120/2500], Loss: 0.2335\n",
            "Epoch [1121/2500], Loss: 0.2336\n",
            "Epoch [1122/2500], Loss: 0.2341\n",
            "Epoch [1123/2500], Loss: 0.2346\n",
            "Epoch [1124/2500], Loss: 0.2357\n",
            "Epoch [1125/2500], Loss: 0.2366\n",
            "Epoch [1126/2500], Loss: 0.2381\n",
            "Epoch [1127/2500], Loss: 0.2385\n",
            "Epoch [1128/2500], Loss: 0.2394\n",
            "Epoch [1129/2500], Loss: 0.2381\n",
            "Epoch [1130/2500], Loss: 0.2373\n",
            "Epoch [1131/2500], Loss: 0.2352\n",
            "Epoch [1132/2500], Loss: 0.2342\n",
            "Epoch [1133/2500], Loss: 0.2331\n",
            "Epoch [1134/2500], Loss: 0.2327\n",
            "Epoch [1135/2500], Loss: 0.2324\n",
            "Epoch [1136/2500], Loss: 0.2322\n",
            "Epoch [1137/2500], Loss: 0.2324\n",
            "Epoch [1138/2500], Loss: 0.2328\n",
            "Epoch [1139/2500], Loss: 0.2338\n",
            "Epoch [1140/2500], Loss: 0.2344\n",
            "Epoch [1141/2500], Loss: 0.2358\n",
            "Epoch [1142/2500], Loss: 0.2355\n",
            "Epoch [1143/2500], Loss: 0.2361\n",
            "Epoch [1144/2500], Loss: 0.2348\n",
            "Epoch [1145/2500], Loss: 0.2343\n",
            "Epoch [1146/2500], Loss: 0.2331\n",
            "Epoch [1147/2500], Loss: 0.2324\n",
            "Epoch [1148/2500], Loss: 0.2317\n",
            "Epoch [1149/2500], Loss: 0.2312\n",
            "Epoch [1150/2500], Loss: 0.2307\n",
            "Epoch [1151/2500], Loss: 0.2303\n",
            "Epoch [1152/2500], Loss: 0.2300\n",
            "Epoch [1153/2500], Loss: 0.2299\n",
            "Epoch [1154/2500], Loss: 0.2299\n",
            "Epoch [1155/2500], Loss: 0.2302\n",
            "Epoch [1156/2500], Loss: 0.2306\n",
            "Epoch [1157/2500], Loss: 0.2311\n",
            "Epoch [1158/2500], Loss: 0.2319\n",
            "Epoch [1159/2500], Loss: 0.2328\n",
            "Epoch [1160/2500], Loss: 0.2345\n",
            "Epoch [1161/2500], Loss: 0.2357\n",
            "Epoch [1162/2500], Loss: 0.2382\n",
            "Epoch [1163/2500], Loss: 0.2381\n",
            "Epoch [1164/2500], Loss: 0.2394\n",
            "Epoch [1165/2500], Loss: 0.2361\n",
            "Epoch [1166/2500], Loss: 0.2343\n",
            "Epoch [1167/2500], Loss: 0.2310\n",
            "Epoch [1168/2500], Loss: 0.2293\n",
            "Epoch [1169/2500], Loss: 0.2288\n",
            "Epoch [1170/2500], Loss: 0.2296\n",
            "Epoch [1171/2500], Loss: 0.2312\n",
            "Epoch [1172/2500], Loss: 0.2324\n",
            "Epoch [1173/2500], Loss: 0.2339\n",
            "Epoch [1174/2500], Loss: 0.2331\n",
            "Epoch [1175/2500], Loss: 0.2326\n",
            "Epoch [1176/2500], Loss: 0.2306\n",
            "Epoch [1177/2500], Loss: 0.2292\n",
            "Epoch [1178/2500], Loss: 0.2282\n",
            "Epoch [1179/2500], Loss: 0.2278\n",
            "Epoch [1180/2500], Loss: 0.2279\n",
            "Epoch [1181/2500], Loss: 0.2281\n",
            "Epoch [1182/2500], Loss: 0.2286\n",
            "Epoch [1183/2500], Loss: 0.2289\n",
            "Epoch [1184/2500], Loss: 0.2296\n",
            "Epoch [1185/2500], Loss: 0.2300\n",
            "Epoch [1186/2500], Loss: 0.2308\n",
            "Epoch [1187/2500], Loss: 0.2311\n",
            "Epoch [1188/2500], Loss: 0.2315\n",
            "Epoch [1189/2500], Loss: 0.2308\n",
            "Epoch [1190/2500], Loss: 0.2302\n",
            "Epoch [1191/2500], Loss: 0.2288\n",
            "Epoch [1192/2500], Loss: 0.2278\n",
            "Epoch [1193/2500], Loss: 0.2270\n",
            "Epoch [1194/2500], Loss: 0.2266\n",
            "Epoch [1195/2500], Loss: 0.2265\n",
            "Epoch [1196/2500], Loss: 0.2265\n",
            "Epoch [1197/2500], Loss: 0.2265\n",
            "Epoch [1198/2500], Loss: 0.2266\n",
            "Epoch [1199/2500], Loss: 0.2267\n",
            "Epoch [1200/2500], Loss: 0.2269\n",
            "Epoch [1201/2500], Loss: 0.2274\n",
            "Epoch [1202/2500], Loss: 0.2280\n",
            "Epoch [1203/2500], Loss: 0.2293\n",
            "Epoch [1204/2500], Loss: 0.2301\n",
            "Epoch [1205/2500], Loss: 0.2322\n",
            "Epoch [1206/2500], Loss: 0.2324\n",
            "Epoch [1207/2500], Loss: 0.2341\n",
            "Epoch [1208/2500], Loss: 0.2324\n",
            "Epoch [1209/2500], Loss: 0.2317\n",
            "Epoch [1210/2500], Loss: 0.2290\n",
            "Epoch [1211/2500], Loss: 0.2270\n",
            "Epoch [1212/2500], Loss: 0.2255\n",
            "Epoch [1213/2500], Loss: 0.2248\n",
            "Epoch [1214/2500], Loss: 0.2250\n",
            "Epoch [1215/2500], Loss: 0.2256\n",
            "Epoch [1216/2500], Loss: 0.2266\n",
            "Epoch [1217/2500], Loss: 0.2274\n",
            "Epoch [1218/2500], Loss: 0.2286\n",
            "Epoch [1219/2500], Loss: 0.2289\n",
            "Epoch [1220/2500], Loss: 0.2294\n",
            "Epoch [1221/2500], Loss: 0.2287\n",
            "Epoch [1222/2500], Loss: 0.2280\n",
            "Epoch [1223/2500], Loss: 0.2265\n",
            "Epoch [1224/2500], Loss: 0.2253\n",
            "Epoch [1225/2500], Loss: 0.2242\n",
            "Epoch [1226/2500], Loss: 0.2236\n",
            "Epoch [1227/2500], Loss: 0.2235\n",
            "Epoch [1228/2500], Loss: 0.2238\n",
            "Epoch [1229/2500], Loss: 0.2242\n",
            "Epoch [1230/2500], Loss: 0.2247\n",
            "Epoch [1231/2500], Loss: 0.2254\n",
            "Epoch [1232/2500], Loss: 0.2260\n",
            "Epoch [1233/2500], Loss: 0.2269\n",
            "Epoch [1234/2500], Loss: 0.2272\n",
            "Epoch [1235/2500], Loss: 0.2281\n",
            "Epoch [1236/2500], Loss: 0.2277\n",
            "Epoch [1237/2500], Loss: 0.2279\n",
            "Epoch [1238/2500], Loss: 0.2268\n",
            "Epoch [1239/2500], Loss: 0.2263\n",
            "Epoch [1240/2500], Loss: 0.2249\n",
            "Epoch [1241/2500], Loss: 0.2241\n",
            "Epoch [1242/2500], Loss: 0.2232\n",
            "Epoch [1243/2500], Loss: 0.2226\n",
            "Epoch [1244/2500], Loss: 0.2222\n",
            "Epoch [1245/2500], Loss: 0.2221\n",
            "Epoch [1246/2500], Loss: 0.2222\n",
            "Epoch [1247/2500], Loss: 0.2225\n",
            "Epoch [1248/2500], Loss: 0.2230\n",
            "Epoch [1249/2500], Loss: 0.2235\n",
            "Epoch [1250/2500], Loss: 0.2245\n",
            "Epoch [1251/2500], Loss: 0.2252\n",
            "Epoch [1252/2500], Loss: 0.2268\n",
            "Epoch [1253/2500], Loss: 0.2274\n",
            "Epoch [1254/2500], Loss: 0.2293\n",
            "Epoch [1255/2500], Loss: 0.2290\n",
            "Epoch [1256/2500], Loss: 0.2295\n",
            "Epoch [1257/2500], Loss: 0.2274\n",
            "Epoch [1258/2500], Loss: 0.2257\n",
            "Epoch [1259/2500], Loss: 0.2232\n",
            "Epoch [1260/2500], Loss: 0.2216\n",
            "Epoch [1261/2500], Loss: 0.2209\n",
            "Epoch [1262/2500], Loss: 0.2211\n",
            "Epoch [1263/2500], Loss: 0.2220\n",
            "Epoch [1264/2500], Loss: 0.2230\n",
            "Epoch [1265/2500], Loss: 0.2243\n",
            "Epoch [1266/2500], Loss: 0.2248\n",
            "Epoch [1267/2500], Loss: 0.2255\n",
            "Epoch [1268/2500], Loss: 0.2249\n",
            "Epoch [1269/2500], Loss: 0.2243\n",
            "Epoch [1270/2500], Loss: 0.2228\n",
            "Epoch [1271/2500], Loss: 0.2216\n",
            "Epoch [1272/2500], Loss: 0.2205\n",
            "Epoch [1273/2500], Loss: 0.2199\n",
            "Epoch [1274/2500], Loss: 0.2197\n",
            "Epoch [1275/2500], Loss: 0.2199\n",
            "Epoch [1276/2500], Loss: 0.2203\n",
            "Epoch [1277/2500], Loss: 0.2209\n",
            "Epoch [1278/2500], Loss: 0.2216\n",
            "Epoch [1279/2500], Loss: 0.2222\n",
            "Epoch [1280/2500], Loss: 0.2231\n",
            "Epoch [1281/2500], Loss: 0.2235\n",
            "Epoch [1282/2500], Loss: 0.2242\n",
            "Epoch [1283/2500], Loss: 0.2238\n",
            "Epoch [1284/2500], Loss: 0.2238\n",
            "Epoch [1285/2500], Loss: 0.2226\n",
            "Epoch [1286/2500], Loss: 0.2220\n",
            "Epoch [1287/2500], Loss: 0.2209\n",
            "Epoch [1288/2500], Loss: 0.2204\n",
            "Epoch [1289/2500], Loss: 0.2198\n",
            "Epoch [1290/2500], Loss: 0.2196\n",
            "Epoch [1291/2500], Loss: 0.2194\n",
            "Epoch [1292/2500], Loss: 0.2194\n",
            "Epoch [1293/2500], Loss: 0.2193\n",
            "Epoch [1294/2500], Loss: 0.2193\n",
            "Epoch [1295/2500], Loss: 0.2193\n",
            "Epoch [1296/2500], Loss: 0.2195\n",
            "Epoch [1297/2500], Loss: 0.2198\n",
            "Epoch [1298/2500], Loss: 0.2202\n",
            "Epoch [1299/2500], Loss: 0.2210\n",
            "Epoch [1300/2500], Loss: 0.2217\n",
            "Epoch [1301/2500], Loss: 0.2233\n",
            "Epoch [1302/2500], Loss: 0.2238\n",
            "Epoch [1303/2500], Loss: 0.2257\n",
            "Epoch [1304/2500], Loss: 0.2248\n",
            "Epoch [1305/2500], Loss: 0.2252\n",
            "Epoch [1306/2500], Loss: 0.2225\n",
            "Epoch [1307/2500], Loss: 0.2208\n",
            "Epoch [1308/2500], Loss: 0.2186\n",
            "Epoch [1309/2500], Loss: 0.2174\n",
            "Epoch [1310/2500], Loss: 0.2170\n",
            "Epoch [1311/2500], Loss: 0.2173\n",
            "Epoch [1312/2500], Loss: 0.2181\n",
            "Epoch [1313/2500], Loss: 0.2188\n",
            "Epoch [1314/2500], Loss: 0.2198\n",
            "Epoch [1315/2500], Loss: 0.2200\n",
            "Epoch [1316/2500], Loss: 0.2207\n",
            "Epoch [1317/2500], Loss: 0.2204\n",
            "Epoch [1318/2500], Loss: 0.2207\n",
            "Epoch [1319/2500], Loss: 0.2204\n",
            "Epoch [1320/2500], Loss: 0.2204\n",
            "Epoch [1321/2500], Loss: 0.2200\n",
            "Epoch [1322/2500], Loss: 0.2195\n",
            "Epoch [1323/2500], Loss: 0.2187\n",
            "Epoch [1324/2500], Loss: 0.2177\n",
            "Epoch [1325/2500], Loss: 0.2168\n",
            "Epoch [1326/2500], Loss: 0.2161\n",
            "Epoch [1327/2500], Loss: 0.2158\n",
            "Epoch [1328/2500], Loss: 0.2159\n",
            "Epoch [1329/2500], Loss: 0.2163\n",
            "Epoch [1330/2500], Loss: 0.2169\n",
            "Epoch [1331/2500], Loss: 0.2176\n",
            "Epoch [1332/2500], Loss: 0.2183\n",
            "Epoch [1333/2500], Loss: 0.2190\n",
            "Epoch [1334/2500], Loss: 0.2194\n",
            "Epoch [1335/2500], Loss: 0.2199\n",
            "Epoch [1336/2500], Loss: 0.2197\n",
            "Epoch [1337/2500], Loss: 0.2201\n",
            "Epoch [1338/2500], Loss: 0.2194\n",
            "Epoch [1339/2500], Loss: 0.2198\n",
            "Epoch [1340/2500], Loss: 0.2187\n",
            "Epoch [1341/2500], Loss: 0.2186\n",
            "Epoch [1342/2500], Loss: 0.2171\n",
            "Epoch [1343/2500], Loss: 0.2161\n",
            "Epoch [1344/2500], Loss: 0.2150\n",
            "Epoch [1345/2500], Loss: 0.2144\n",
            "Epoch [1346/2500], Loss: 0.2143\n",
            "Epoch [1347/2500], Loss: 0.2145\n",
            "Epoch [1348/2500], Loss: 0.2149\n",
            "Epoch [1349/2500], Loss: 0.2153\n",
            "Epoch [1350/2500], Loss: 0.2158\n",
            "Epoch [1351/2500], Loss: 0.2159\n",
            "Epoch [1352/2500], Loss: 0.2166\n",
            "Epoch [1353/2500], Loss: 0.2169\n",
            "Epoch [1354/2500], Loss: 0.2181\n",
            "Epoch [1355/2500], Loss: 0.2192\n",
            "Epoch [1356/2500], Loss: 0.2214\n",
            "Epoch [1357/2500], Loss: 0.2223\n",
            "Epoch [1358/2500], Loss: 0.2234\n",
            "Epoch [1359/2500], Loss: 0.2213\n",
            "Epoch [1360/2500], Loss: 0.2189\n",
            "Epoch [1361/2500], Loss: 0.2157\n",
            "Epoch [1362/2500], Loss: 0.2139\n",
            "Epoch [1363/2500], Loss: 0.2136\n",
            "Epoch [1364/2500], Loss: 0.2144\n",
            "Epoch [1365/2500], Loss: 0.2154\n",
            "Epoch [1366/2500], Loss: 0.2160\n",
            "Epoch [1367/2500], Loss: 0.2164\n",
            "Epoch [1368/2500], Loss: 0.2162\n",
            "Epoch [1369/2500], Loss: 0.2166\n",
            "Epoch [1370/2500], Loss: 0.2161\n",
            "Epoch [1371/2500], Loss: 0.2163\n",
            "Epoch [1372/2500], Loss: 0.2151\n",
            "Epoch [1373/2500], Loss: 0.2144\n",
            "Epoch [1374/2500], Loss: 0.2132\n",
            "Epoch [1375/2500], Loss: 0.2124\n",
            "Epoch [1376/2500], Loss: 0.2121\n",
            "Epoch [1377/2500], Loss: 0.2122\n",
            "Epoch [1378/2500], Loss: 0.2126\n",
            "Epoch [1379/2500], Loss: 0.2129\n",
            "Epoch [1380/2500], Loss: 0.2133\n",
            "Epoch [1381/2500], Loss: 0.2133\n",
            "Epoch [1382/2500], Loss: 0.2136\n",
            "Epoch [1383/2500], Loss: 0.2138\n",
            "Epoch [1384/2500], Loss: 0.2146\n",
            "Epoch [1385/2500], Loss: 0.2154\n",
            "Epoch [1386/2500], Loss: 0.2168\n",
            "Epoch [1387/2500], Loss: 0.2176\n",
            "Epoch [1388/2500], Loss: 0.2186\n",
            "Epoch [1389/2500], Loss: 0.2180\n",
            "Epoch [1390/2500], Loss: 0.2174\n",
            "Epoch [1391/2500], Loss: 0.2154\n",
            "Epoch [1392/2500], Loss: 0.2142\n",
            "Epoch [1393/2500], Loss: 0.2128\n",
            "Epoch [1394/2500], Loss: 0.2122\n",
            "Epoch [1395/2500], Loss: 0.2117\n",
            "Epoch [1396/2500], Loss: 0.2114\n",
            "Epoch [1397/2500], Loss: 0.2113\n",
            "Epoch [1398/2500], Loss: 0.2116\n",
            "Epoch [1399/2500], Loss: 0.2123\n",
            "Epoch [1400/2500], Loss: 0.2130\n",
            "Epoch [1401/2500], Loss: 0.2141\n",
            "Epoch [1402/2500], Loss: 0.2142\n",
            "Epoch [1403/2500], Loss: 0.2150\n",
            "Epoch [1404/2500], Loss: 0.2144\n",
            "Epoch [1405/2500], Loss: 0.2144\n",
            "Epoch [1406/2500], Loss: 0.2135\n",
            "Epoch [1407/2500], Loss: 0.2132\n",
            "Epoch [1408/2500], Loss: 0.2124\n",
            "Epoch [1409/2500], Loss: 0.2118\n",
            "Epoch [1410/2500], Loss: 0.2111\n",
            "Epoch [1411/2500], Loss: 0.2104\n",
            "Epoch [1412/2500], Loss: 0.2098\n",
            "Epoch [1413/2500], Loss: 0.2095\n",
            "Epoch [1414/2500], Loss: 0.2094\n",
            "Epoch [1415/2500], Loss: 0.2095\n",
            "Epoch [1416/2500], Loss: 0.2096\n",
            "Epoch [1417/2500], Loss: 0.2097\n",
            "Epoch [1418/2500], Loss: 0.2099\n",
            "Epoch [1419/2500], Loss: 0.2101\n",
            "Epoch [1420/2500], Loss: 0.2106\n",
            "Epoch [1421/2500], Loss: 0.2112\n",
            "Epoch [1422/2500], Loss: 0.2123\n",
            "Epoch [1423/2500], Loss: 0.2136\n",
            "Epoch [1424/2500], Loss: 0.2163\n",
            "Epoch [1425/2500], Loss: 0.2177\n",
            "Epoch [1426/2500], Loss: 0.2211\n",
            "Epoch [1427/2500], Loss: 0.2195\n",
            "Epoch [1428/2500], Loss: 0.2191\n",
            "Epoch [1429/2500], Loss: 0.2143\n",
            "Epoch [1430/2500], Loss: 0.2109\n",
            "Epoch [1431/2500], Loss: 0.2087\n",
            "Epoch [1432/2500], Loss: 0.2084\n",
            "Epoch [1433/2500], Loss: 0.2098\n",
            "Epoch [1434/2500], Loss: 0.2115\n",
            "Epoch [1435/2500], Loss: 0.2136\n",
            "Epoch [1436/2500], Loss: 0.2138\n",
            "Epoch [1437/2500], Loss: 0.2139\n",
            "Epoch [1438/2500], Loss: 0.2121\n",
            "Epoch [1439/2500], Loss: 0.2104\n",
            "Epoch [1440/2500], Loss: 0.2088\n",
            "Epoch [1441/2500], Loss: 0.2079\n",
            "Epoch [1442/2500], Loss: 0.2077\n",
            "Epoch [1443/2500], Loss: 0.2081\n",
            "Epoch [1444/2500], Loss: 0.2089\n",
            "Epoch [1445/2500], Loss: 0.2096\n",
            "Epoch [1446/2500], Loss: 0.2105\n",
            "Epoch [1447/2500], Loss: 0.2107\n",
            "Epoch [1448/2500], Loss: 0.2110\n",
            "Epoch [1449/2500], Loss: 0.2104\n",
            "Epoch [1450/2500], Loss: 0.2099\n",
            "Epoch [1451/2500], Loss: 0.2089\n",
            "Epoch [1452/2500], Loss: 0.2080\n",
            "Epoch [1453/2500], Loss: 0.2073\n",
            "Epoch [1454/2500], Loss: 0.2068\n",
            "Epoch [1455/2500], Loss: 0.2066\n",
            "Epoch [1456/2500], Loss: 0.2066\n",
            "Epoch [1457/2500], Loss: 0.2068\n",
            "Epoch [1458/2500], Loss: 0.2070\n",
            "Epoch [1459/2500], Loss: 0.2074\n",
            "Epoch [1460/2500], Loss: 0.2078\n",
            "Epoch [1461/2500], Loss: 0.2084\n",
            "Epoch [1462/2500], Loss: 0.2089\n",
            "Epoch [1463/2500], Loss: 0.2098\n",
            "Epoch [1464/2500], Loss: 0.2104\n",
            "Epoch [1465/2500], Loss: 0.2114\n",
            "Epoch [1466/2500], Loss: 0.2115\n",
            "Epoch [1467/2500], Loss: 0.2120\n",
            "Epoch [1468/2500], Loss: 0.2110\n",
            "Epoch [1469/2500], Loss: 0.2105\n",
            "Epoch [1470/2500], Loss: 0.2089\n",
            "Epoch [1471/2500], Loss: 0.2078\n",
            "Epoch [1472/2500], Loss: 0.2066\n",
            "Epoch [1473/2500], Loss: 0.2058\n",
            "Epoch [1474/2500], Loss: 0.2054\n",
            "Epoch [1475/2500], Loss: 0.2054\n",
            "Epoch [1476/2500], Loss: 0.2056\n",
            "Epoch [1477/2500], Loss: 0.2060\n",
            "Epoch [1478/2500], Loss: 0.2066\n",
            "Epoch [1479/2500], Loss: 0.2072\n",
            "Epoch [1480/2500], Loss: 0.2082\n",
            "Epoch [1481/2500], Loss: 0.2089\n",
            "Epoch [1482/2500], Loss: 0.2102\n",
            "Epoch [1483/2500], Loss: 0.2106\n",
            "Epoch [1484/2500], Loss: 0.2117\n",
            "Epoch [1485/2500], Loss: 0.2110\n",
            "Epoch [1486/2500], Loss: 0.2107\n",
            "Epoch [1487/2500], Loss: 0.2089\n",
            "Epoch [1488/2500], Loss: 0.2074\n",
            "Epoch [1489/2500], Loss: 0.2058\n",
            "Epoch [1490/2500], Loss: 0.2047\n",
            "Epoch [1491/2500], Loss: 0.2043\n",
            "Epoch [1492/2500], Loss: 0.2043\n",
            "Epoch [1493/2500], Loss: 0.2047\n",
            "Epoch [1494/2500], Loss: 0.2053\n",
            "Epoch [1495/2500], Loss: 0.2062\n",
            "Epoch [1496/2500], Loss: 0.2069\n",
            "Epoch [1497/2500], Loss: 0.2079\n",
            "Epoch [1498/2500], Loss: 0.2083\n",
            "Epoch [1499/2500], Loss: 0.2091\n",
            "Epoch [1500/2500], Loss: 0.2087\n",
            "Epoch [1501/2500], Loss: 0.2085\n",
            "Epoch [1502/2500], Loss: 0.2072\n",
            "Epoch [1503/2500], Loss: 0.2062\n",
            "Epoch [1504/2500], Loss: 0.2049\n",
            "Epoch [1505/2500], Loss: 0.2040\n",
            "Epoch [1506/2500], Loss: 0.2034\n",
            "Epoch [1507/2500], Loss: 0.2032\n",
            "Epoch [1508/2500], Loss: 0.2032\n",
            "Epoch [1509/2500], Loss: 0.2035\n",
            "Epoch [1510/2500], Loss: 0.2038\n",
            "Epoch [1511/2500], Loss: 0.2043\n",
            "Epoch [1512/2500], Loss: 0.2050\n",
            "Epoch [1513/2500], Loss: 0.2056\n",
            "Epoch [1514/2500], Loss: 0.2066\n",
            "Epoch [1515/2500], Loss: 0.2073\n",
            "Epoch [1516/2500], Loss: 0.2084\n",
            "Epoch [1517/2500], Loss: 0.2085\n",
            "Epoch [1518/2500], Loss: 0.2088\n",
            "Epoch [1519/2500], Loss: 0.2078\n",
            "Epoch [1520/2500], Loss: 0.2072\n",
            "Epoch [1521/2500], Loss: 0.2056\n",
            "Epoch [1522/2500], Loss: 0.2049\n",
            "Epoch [1523/2500], Loss: 0.2040\n",
            "Epoch [1524/2500], Loss: 0.2040\n",
            "Epoch [1525/2500], Loss: 0.2038\n",
            "Epoch [1526/2500], Loss: 0.2041\n",
            "Epoch [1527/2500], Loss: 0.2041\n",
            "Epoch [1528/2500], Loss: 0.2040\n",
            "Epoch [1529/2500], Loss: 0.2039\n",
            "Epoch [1530/2500], Loss: 0.2037\n",
            "Epoch [1531/2500], Loss: 0.2037\n",
            "Epoch [1532/2500], Loss: 0.2038\n",
            "Epoch [1533/2500], Loss: 0.2044\n",
            "Epoch [1534/2500], Loss: 0.2047\n",
            "Epoch [1535/2500], Loss: 0.2058\n",
            "Epoch [1536/2500], Loss: 0.2057\n",
            "Epoch [1537/2500], Loss: 0.2065\n",
            "Epoch [1538/2500], Loss: 0.2053\n",
            "Epoch [1539/2500], Loss: 0.2049\n",
            "Epoch [1540/2500], Loss: 0.2035\n",
            "Epoch [1541/2500], Loss: 0.2027\n",
            "Epoch [1542/2500], Loss: 0.2022\n",
            "Epoch [1543/2500], Loss: 0.2022\n",
            "Epoch [1544/2500], Loss: 0.2024\n",
            "Epoch [1545/2500], Loss: 0.2025\n",
            "Epoch [1546/2500], Loss: 0.2025\n",
            "Epoch [1547/2500], Loss: 0.2022\n",
            "Epoch [1548/2500], Loss: 0.2017\n",
            "Epoch [1549/2500], Loss: 0.2011\n",
            "Epoch [1550/2500], Loss: 0.2007\n",
            "Epoch [1551/2500], Loss: 0.2005\n",
            "Epoch [1552/2500], Loss: 0.2005\n",
            "Epoch [1553/2500], Loss: 0.2006\n",
            "Epoch [1554/2500], Loss: 0.2008\n",
            "Epoch [1555/2500], Loss: 0.2011\n",
            "Epoch [1556/2500], Loss: 0.2011\n",
            "Epoch [1557/2500], Loss: 0.2014\n",
            "Epoch [1558/2500], Loss: 0.2015\n",
            "Epoch [1559/2500], Loss: 0.2021\n",
            "Epoch [1560/2500], Loss: 0.2027\n",
            "Epoch [1561/2500], Loss: 0.2047\n",
            "Epoch [1562/2500], Loss: 0.2070\n",
            "Epoch [1563/2500], Loss: 0.2117\n",
            "Epoch [1564/2500], Loss: 0.2148\n",
            "Epoch [1565/2500], Loss: 0.2186\n",
            "Epoch [1566/2500], Loss: 0.2140\n",
            "Epoch [1567/2500], Loss: 0.2088\n",
            "Epoch [1568/2500], Loss: 0.2022\n",
            "Epoch [1569/2500], Loss: 0.2003\n",
            "Epoch [1570/2500], Loss: 0.2021\n",
            "Epoch [1571/2500], Loss: 0.2049\n",
            "Epoch [1572/2500], Loss: 0.2066\n",
            "Epoch [1573/2500], Loss: 0.2051\n",
            "Epoch [1574/2500], Loss: 0.2037\n",
            "Epoch [1575/2500], Loss: 0.2018\n",
            "Epoch [1576/2500], Loss: 0.2015\n",
            "Epoch [1577/2500], Loss: 0.2010\n",
            "Epoch [1578/2500], Loss: 0.2007\n",
            "Epoch [1579/2500], Loss: 0.2005\n",
            "Epoch [1580/2500], Loss: 0.2007\n",
            "Epoch [1581/2500], Loss: 0.2016\n",
            "Epoch [1582/2500], Loss: 0.2016\n",
            "Epoch [1583/2500], Loss: 0.2018\n",
            "Epoch [1584/2500], Loss: 0.2003\n",
            "Epoch [1585/2500], Loss: 0.1991\n",
            "Epoch [1586/2500], Loss: 0.1983\n",
            "Epoch [1587/2500], Loss: 0.1983\n",
            "Epoch [1588/2500], Loss: 0.1988\n",
            "Epoch [1589/2500], Loss: 0.1992\n",
            "Epoch [1590/2500], Loss: 0.1995\n",
            "Epoch [1591/2500], Loss: 0.1992\n",
            "Epoch [1592/2500], Loss: 0.1990\n",
            "Epoch [1593/2500], Loss: 0.1989\n",
            "Epoch [1594/2500], Loss: 0.1992\n",
            "Epoch [1595/2500], Loss: 0.1994\n",
            "Epoch [1596/2500], Loss: 0.1997\n",
            "Epoch [1597/2500], Loss: 0.1997\n",
            "Epoch [1598/2500], Loss: 0.1996\n",
            "Epoch [1599/2500], Loss: 0.1993\n",
            "Epoch [1600/2500], Loss: 0.1992\n",
            "Epoch [1601/2500], Loss: 0.1991\n",
            "Epoch [1602/2500], Loss: 0.1993\n",
            "Epoch [1603/2500], Loss: 0.1992\n",
            "Epoch [1604/2500], Loss: 0.1994\n",
            "Epoch [1605/2500], Loss: 0.1990\n",
            "Epoch [1606/2500], Loss: 0.1990\n",
            "Epoch [1607/2500], Loss: 0.1987\n",
            "Epoch [1608/2500], Loss: 0.1987\n",
            "Epoch [1609/2500], Loss: 0.1987\n",
            "Epoch [1610/2500], Loss: 0.1989\n",
            "Epoch [1611/2500], Loss: 0.1990\n",
            "Epoch [1612/2500], Loss: 0.1992\n",
            "Epoch [1613/2500], Loss: 0.1991\n",
            "Epoch [1614/2500], Loss: 0.1993\n",
            "Epoch [1615/2500], Loss: 0.1993\n",
            "Epoch [1616/2500], Loss: 0.1998\n",
            "Epoch [1617/2500], Loss: 0.1999\n",
            "Epoch [1618/2500], Loss: 0.2006\n",
            "Epoch [1619/2500], Loss: 0.2003\n",
            "Epoch [1620/2500], Loss: 0.2007\n",
            "Epoch [1621/2500], Loss: 0.2001\n",
            "Epoch [1622/2500], Loss: 0.2000\n",
            "Epoch [1623/2500], Loss: 0.1994\n",
            "Epoch [1624/2500], Loss: 0.1990\n",
            "Epoch [1625/2500], Loss: 0.1983\n",
            "Epoch [1626/2500], Loss: 0.1978\n",
            "Epoch [1627/2500], Loss: 0.1971\n",
            "Epoch [1628/2500], Loss: 0.1965\n",
            "Epoch [1629/2500], Loss: 0.1961\n",
            "Epoch [1630/2500], Loss: 0.1958\n",
            "Epoch [1631/2500], Loss: 0.1956\n",
            "Epoch [1632/2500], Loss: 0.1955\n",
            "Epoch [1633/2500], Loss: 0.1954\n",
            "Epoch [1634/2500], Loss: 0.1954\n",
            "Epoch [1635/2500], Loss: 0.1953\n",
            "Epoch [1636/2500], Loss: 0.1953\n",
            "Epoch [1637/2500], Loss: 0.1953\n",
            "Epoch [1638/2500], Loss: 0.1953\n",
            "Epoch [1639/2500], Loss: 0.1954\n",
            "Epoch [1640/2500], Loss: 0.1956\n",
            "Epoch [1641/2500], Loss: 0.1962\n",
            "Epoch [1642/2500], Loss: 0.1970\n",
            "Epoch [1643/2500], Loss: 0.1987\n",
            "Epoch [1644/2500], Loss: 0.2009\n",
            "Epoch [1645/2500], Loss: 0.2056\n",
            "Epoch [1646/2500], Loss: 0.2091\n",
            "Epoch [1647/2500], Loss: 0.2158\n",
            "Epoch [1648/2500], Loss: 0.2128\n",
            "Epoch [1649/2500], Loss: 0.2099\n",
            "Epoch [1650/2500], Loss: 0.2005\n",
            "Epoch [1651/2500], Loss: 0.1953\n",
            "Epoch [1652/2500], Loss: 0.1953\n",
            "Epoch [1653/2500], Loss: 0.1989\n",
            "Epoch [1654/2500], Loss: 0.2033\n",
            "Epoch [1655/2500], Loss: 0.2030\n",
            "Epoch [1656/2500], Loss: 0.2013\n",
            "Epoch [1657/2500], Loss: 0.1969\n",
            "Epoch [1658/2500], Loss: 0.1947\n",
            "Epoch [1659/2500], Loss: 0.1947\n",
            "Epoch [1660/2500], Loss: 0.1963\n",
            "Epoch [1661/2500], Loss: 0.1982\n",
            "Epoch [1662/2500], Loss: 0.1984\n",
            "Epoch [1663/2500], Loss: 0.1980\n",
            "Epoch [1664/2500], Loss: 0.1960\n",
            "Epoch [1665/2500], Loss: 0.1947\n",
            "Epoch [1666/2500], Loss: 0.1939\n",
            "Epoch [1667/2500], Loss: 0.1940\n",
            "Epoch [1668/2500], Loss: 0.1946\n",
            "Epoch [1669/2500], Loss: 0.1953\n",
            "Epoch [1670/2500], Loss: 0.1960\n",
            "Epoch [1671/2500], Loss: 0.1958\n",
            "Epoch [1672/2500], Loss: 0.1956\n",
            "Epoch [1673/2500], Loss: 0.1947\n",
            "Epoch [1674/2500], Loss: 0.1939\n",
            "Epoch [1675/2500], Loss: 0.1933\n",
            "Epoch [1676/2500], Loss: 0.1929\n",
            "Epoch [1677/2500], Loss: 0.1929\n",
            "Epoch [1678/2500], Loss: 0.1931\n",
            "Epoch [1679/2500], Loss: 0.1934\n",
            "Epoch [1680/2500], Loss: 0.1937\n",
            "Epoch [1681/2500], Loss: 0.1940\n",
            "Epoch [1682/2500], Loss: 0.1941\n",
            "Epoch [1683/2500], Loss: 0.1943\n",
            "Epoch [1684/2500], Loss: 0.1942\n",
            "Epoch [1685/2500], Loss: 0.1941\n",
            "Epoch [1686/2500], Loss: 0.1939\n",
            "Epoch [1687/2500], Loss: 0.1938\n",
            "Epoch [1688/2500], Loss: 0.1936\n",
            "Epoch [1689/2500], Loss: 0.1934\n",
            "Epoch [1690/2500], Loss: 0.1932\n",
            "Epoch [1691/2500], Loss: 0.1930\n",
            "Epoch [1692/2500], Loss: 0.1927\n",
            "Epoch [1693/2500], Loss: 0.1926\n",
            "Epoch [1694/2500], Loss: 0.1924\n",
            "Epoch [1695/2500], Loss: 0.1923\n",
            "Epoch [1696/2500], Loss: 0.1922\n",
            "Epoch [1697/2500], Loss: 0.1922\n",
            "Epoch [1698/2500], Loss: 0.1922\n",
            "Epoch [1699/2500], Loss: 0.1923\n",
            "Epoch [1700/2500], Loss: 0.1924\n",
            "Epoch [1701/2500], Loss: 0.1928\n",
            "Epoch [1702/2500], Loss: 0.1932\n",
            "Epoch [1703/2500], Loss: 0.1940\n",
            "Epoch [1704/2500], Loss: 0.1950\n",
            "Epoch [1705/2500], Loss: 0.1968\n",
            "Epoch [1706/2500], Loss: 0.1983\n",
            "Epoch [1707/2500], Loss: 0.2010\n",
            "Epoch [1708/2500], Loss: 0.2017\n",
            "Epoch [1709/2500], Loss: 0.2028\n",
            "Epoch [1710/2500], Loss: 0.1999\n",
            "Epoch [1711/2500], Loss: 0.1972\n",
            "Epoch [1712/2500], Loss: 0.1934\n",
            "Epoch [1713/2500], Loss: 0.1914\n",
            "Epoch [1714/2500], Loss: 0.1910\n",
            "Epoch [1715/2500], Loss: 0.1921\n",
            "Epoch [1716/2500], Loss: 0.1938\n",
            "Epoch [1717/2500], Loss: 0.1952\n",
            "Epoch [1718/2500], Loss: 0.1961\n",
            "Epoch [1719/2500], Loss: 0.1955\n",
            "Epoch [1720/2500], Loss: 0.1947\n",
            "Epoch [1721/2500], Loss: 0.1929\n",
            "Epoch [1722/2500], Loss: 0.1917\n",
            "Epoch [1723/2500], Loss: 0.1909\n",
            "Epoch [1724/2500], Loss: 0.1907\n",
            "Epoch [1725/2500], Loss: 0.1910\n",
            "Epoch [1726/2500], Loss: 0.1915\n",
            "Epoch [1727/2500], Loss: 0.1920\n",
            "Epoch [1728/2500], Loss: 0.1924\n",
            "Epoch [1729/2500], Loss: 0.1927\n",
            "Epoch [1730/2500], Loss: 0.1926\n",
            "Epoch [1731/2500], Loss: 0.1926\n",
            "Epoch [1732/2500], Loss: 0.1922\n",
            "Epoch [1733/2500], Loss: 0.1920\n",
            "Epoch [1734/2500], Loss: 0.1915\n",
            "Epoch [1735/2500], Loss: 0.1914\n",
            "Epoch [1736/2500], Loss: 0.1910\n",
            "Epoch [1737/2500], Loss: 0.1908\n",
            "Epoch [1738/2500], Loss: 0.1905\n",
            "Epoch [1739/2500], Loss: 0.1903\n",
            "Epoch [1740/2500], Loss: 0.1899\n",
            "Epoch [1741/2500], Loss: 0.1897\n",
            "Epoch [1742/2500], Loss: 0.1894\n",
            "Epoch [1743/2500], Loss: 0.1893\n",
            "Epoch [1744/2500], Loss: 0.1892\n",
            "Epoch [1745/2500], Loss: 0.1892\n",
            "Epoch [1746/2500], Loss: 0.1894\n",
            "Epoch [1747/2500], Loss: 0.1896\n",
            "Epoch [1748/2500], Loss: 0.1900\n",
            "Epoch [1749/2500], Loss: 0.1905\n",
            "Epoch [1750/2500], Loss: 0.1916\n",
            "Epoch [1751/2500], Loss: 0.1926\n",
            "Epoch [1752/2500], Loss: 0.1948\n",
            "Epoch [1753/2500], Loss: 0.1964\n",
            "Epoch [1754/2500], Loss: 0.1996\n",
            "Epoch [1755/2500], Loss: 0.2000\n",
            "Epoch [1756/2500], Loss: 0.2011\n",
            "Epoch [1757/2500], Loss: 0.1977\n",
            "Epoch [1758/2500], Loss: 0.1944\n",
            "Epoch [1759/2500], Loss: 0.1906\n",
            "Epoch [1760/2500], Loss: 0.1886\n",
            "Epoch [1761/2500], Loss: 0.1887\n",
            "Epoch [1762/2500], Loss: 0.1901\n",
            "Epoch [1763/2500], Loss: 0.1923\n",
            "Epoch [1764/2500], Loss: 0.1934\n",
            "Epoch [1765/2500], Loss: 0.1943\n",
            "Epoch [1766/2500], Loss: 0.1929\n",
            "Epoch [1767/2500], Loss: 0.1914\n",
            "Epoch [1768/2500], Loss: 0.1894\n",
            "Epoch [1769/2500], Loss: 0.1881\n",
            "Epoch [1770/2500], Loss: 0.1877\n",
            "Epoch [1771/2500], Loss: 0.1880\n",
            "Epoch [1772/2500], Loss: 0.1889\n",
            "Epoch [1773/2500], Loss: 0.1897\n",
            "Epoch [1774/2500], Loss: 0.1906\n",
            "Epoch [1775/2500], Loss: 0.1906\n",
            "Epoch [1776/2500], Loss: 0.1908\n",
            "Epoch [1777/2500], Loss: 0.1902\n",
            "Epoch [1778/2500], Loss: 0.1896\n",
            "Epoch [1779/2500], Loss: 0.1888\n",
            "Epoch [1780/2500], Loss: 0.1882\n",
            "Epoch [1781/2500], Loss: 0.1877\n",
            "Epoch [1782/2500], Loss: 0.1874\n",
            "Epoch [1783/2500], Loss: 0.1873\n",
            "Epoch [1784/2500], Loss: 0.1873\n",
            "Epoch [1785/2500], Loss: 0.1875\n",
            "Epoch [1786/2500], Loss: 0.1876\n",
            "Epoch [1787/2500], Loss: 0.1879\n",
            "Epoch [1788/2500], Loss: 0.1879\n",
            "Epoch [1789/2500], Loss: 0.1881\n",
            "Epoch [1790/2500], Loss: 0.1881\n",
            "Epoch [1791/2500], Loss: 0.1883\n",
            "Epoch [1792/2500], Loss: 0.1883\n",
            "Epoch [1793/2500], Loss: 0.1886\n",
            "Epoch [1794/2500], Loss: 0.1888\n",
            "Epoch [1795/2500], Loss: 0.1894\n",
            "Epoch [1796/2500], Loss: 0.1899\n",
            "Epoch [1797/2500], Loss: 0.1908\n",
            "Epoch [1798/2500], Loss: 0.1915\n",
            "Epoch [1799/2500], Loss: 0.1923\n",
            "Epoch [1800/2500], Loss: 0.1925\n",
            "Epoch [1801/2500], Loss: 0.1921\n",
            "Epoch [1802/2500], Loss: 0.1908\n",
            "Epoch [1803/2500], Loss: 0.1890\n",
            "Epoch [1804/2500], Loss: 0.1872\n",
            "Epoch [1805/2500], Loss: 0.1860\n",
            "Epoch [1806/2500], Loss: 0.1857\n",
            "Epoch [1807/2500], Loss: 0.1861\n",
            "Epoch [1808/2500], Loss: 0.1869\n",
            "Epoch [1809/2500], Loss: 0.1878\n",
            "Epoch [1810/2500], Loss: 0.1887\n",
            "Epoch [1811/2500], Loss: 0.1893\n",
            "Epoch [1812/2500], Loss: 0.1896\n",
            "Epoch [1813/2500], Loss: 0.1894\n",
            "Epoch [1814/2500], Loss: 0.1889\n",
            "Epoch [1815/2500], Loss: 0.1881\n",
            "Epoch [1816/2500], Loss: 0.1877\n",
            "Epoch [1817/2500], Loss: 0.1872\n",
            "Epoch [1818/2500], Loss: 0.1875\n",
            "Epoch [1819/2500], Loss: 0.1874\n",
            "Epoch [1820/2500], Loss: 0.1878\n",
            "Epoch [1821/2500], Loss: 0.1874\n",
            "Epoch [1822/2500], Loss: 0.1873\n",
            "Epoch [1823/2500], Loss: 0.1864\n",
            "Epoch [1824/2500], Loss: 0.1857\n",
            "Epoch [1825/2500], Loss: 0.1850\n",
            "Epoch [1826/2500], Loss: 0.1846\n",
            "Epoch [1827/2500], Loss: 0.1846\n",
            "Epoch [1828/2500], Loss: 0.1848\n",
            "Epoch [1829/2500], Loss: 0.1852\n",
            "Epoch [1830/2500], Loss: 0.1855\n",
            "Epoch [1831/2500], Loss: 0.1860\n",
            "Epoch [1832/2500], Loss: 0.1861\n",
            "Epoch [1833/2500], Loss: 0.1866\n",
            "Epoch [1834/2500], Loss: 0.1866\n",
            "Epoch [1835/2500], Loss: 0.1873\n",
            "Epoch [1836/2500], Loss: 0.1879\n",
            "Epoch [1837/2500], Loss: 0.1898\n",
            "Epoch [1838/2500], Loss: 0.1918\n",
            "Epoch [1839/2500], Loss: 0.1953\n",
            "Epoch [1840/2500], Loss: 0.1971\n",
            "Epoch [1841/2500], Loss: 0.1984\n",
            "Epoch [1842/2500], Loss: 0.1948\n",
            "Epoch [1843/2500], Loss: 0.1904\n",
            "Epoch [1844/2500], Loss: 0.1858\n",
            "Epoch [1845/2500], Loss: 0.1843\n",
            "Epoch [1846/2500], Loss: 0.1854\n",
            "Epoch [1847/2500], Loss: 0.1876\n",
            "Epoch [1848/2500], Loss: 0.1891\n",
            "Epoch [1849/2500], Loss: 0.1890\n",
            "Epoch [1850/2500], Loss: 0.1883\n",
            "Epoch [1851/2500], Loss: 0.1866\n",
            "Epoch [1852/2500], Loss: 0.1861\n",
            "Epoch [1853/2500], Loss: 0.1853\n",
            "Epoch [1854/2500], Loss: 0.1852\n",
            "Epoch [1855/2500], Loss: 0.1848\n",
            "Epoch [1856/2500], Loss: 0.1844\n",
            "Epoch [1857/2500], Loss: 0.1843\n",
            "Epoch [1858/2500], Loss: 0.1845\n",
            "Epoch [1859/2500], Loss: 0.1852\n",
            "Epoch [1860/2500], Loss: 0.1854\n",
            "Epoch [1861/2500], Loss: 0.1859\n",
            "Epoch [1862/2500], Loss: 0.1851\n",
            "Epoch [1863/2500], Loss: 0.1845\n",
            "Epoch [1864/2500], Loss: 0.1836\n",
            "Epoch [1865/2500], Loss: 0.1831\n",
            "Epoch [1866/2500], Loss: 0.1830\n",
            "Epoch [1867/2500], Loss: 0.1831\n",
            "Epoch [1868/2500], Loss: 0.1832\n",
            "Epoch [1869/2500], Loss: 0.1831\n",
            "Epoch [1870/2500], Loss: 0.1830\n",
            "Epoch [1871/2500], Loss: 0.1826\n",
            "Epoch [1872/2500], Loss: 0.1824\n",
            "Epoch [1873/2500], Loss: 0.1822\n",
            "Epoch [1874/2500], Loss: 0.1822\n",
            "Epoch [1875/2500], Loss: 0.1823\n",
            "Epoch [1876/2500], Loss: 0.1824\n",
            "Epoch [1877/2500], Loss: 0.1826\n",
            "Epoch [1878/2500], Loss: 0.1827\n",
            "Epoch [1879/2500], Loss: 0.1829\n",
            "Epoch [1880/2500], Loss: 0.1832\n",
            "Epoch [1881/2500], Loss: 0.1837\n",
            "Epoch [1882/2500], Loss: 0.1846\n",
            "Epoch [1883/2500], Loss: 0.1858\n",
            "Epoch [1884/2500], Loss: 0.1880\n",
            "Epoch [1885/2500], Loss: 0.1900\n",
            "Epoch [1886/2500], Loss: 0.1941\n",
            "Epoch [1887/2500], Loss: 0.1945\n",
            "Epoch [1888/2500], Loss: 0.1969\n",
            "Epoch [1889/2500], Loss: 0.1923\n",
            "Epoch [1890/2500], Loss: 0.1886\n",
            "Epoch [1891/2500], Loss: 0.1837\n",
            "Epoch [1892/2500], Loss: 0.1815\n",
            "Epoch [1893/2500], Loss: 0.1820\n",
            "Epoch [1894/2500], Loss: 0.1842\n",
            "Epoch [1895/2500], Loss: 0.1870\n",
            "Epoch [1896/2500], Loss: 0.1877\n",
            "Epoch [1897/2500], Loss: 0.1879\n",
            "Epoch [1898/2500], Loss: 0.1856\n",
            "Epoch [1899/2500], Loss: 0.1835\n",
            "Epoch [1900/2500], Loss: 0.1817\n",
            "Epoch [1901/2500], Loss: 0.1810\n",
            "Epoch [1902/2500], Loss: 0.1813\n",
            "Epoch [1903/2500], Loss: 0.1822\n",
            "Epoch [1904/2500], Loss: 0.1834\n",
            "Epoch [1905/2500], Loss: 0.1839\n",
            "Epoch [1906/2500], Loss: 0.1843\n",
            "Epoch [1907/2500], Loss: 0.1838\n",
            "Epoch [1908/2500], Loss: 0.1831\n",
            "Epoch [1909/2500], Loss: 0.1821\n",
            "Epoch [1910/2500], Loss: 0.1813\n",
            "Epoch [1911/2500], Loss: 0.1806\n",
            "Epoch [1912/2500], Loss: 0.1803\n",
            "Epoch [1913/2500], Loss: 0.1803\n",
            "Epoch [1914/2500], Loss: 0.1805\n",
            "Epoch [1915/2500], Loss: 0.1808\n",
            "Epoch [1916/2500], Loss: 0.1812\n",
            "Epoch [1917/2500], Loss: 0.1816\n",
            "Epoch [1918/2500], Loss: 0.1820\n",
            "Epoch [1919/2500], Loss: 0.1824\n",
            "Epoch [1920/2500], Loss: 0.1826\n",
            "Epoch [1921/2500], Loss: 0.1827\n",
            "Epoch [1922/2500], Loss: 0.1826\n",
            "Epoch [1923/2500], Loss: 0.1824\n",
            "Epoch [1924/2500], Loss: 0.1820\n",
            "Epoch [1925/2500], Loss: 0.1816\n",
            "Epoch [1926/2500], Loss: 0.1811\n",
            "Epoch [1927/2500], Loss: 0.1808\n",
            "Epoch [1928/2500], Loss: 0.1804\n",
            "Epoch [1929/2500], Loss: 0.1801\n",
            "Epoch [1930/2500], Loss: 0.1798\n",
            "Epoch [1931/2500], Loss: 0.1796\n",
            "Epoch [1932/2500], Loss: 0.1794\n",
            "Epoch [1933/2500], Loss: 0.1792\n",
            "Epoch [1934/2500], Loss: 0.1791\n",
            "Epoch [1935/2500], Loss: 0.1790\n",
            "Epoch [1936/2500], Loss: 0.1790\n",
            "Epoch [1937/2500], Loss: 0.1789\n",
            "Epoch [1938/2500], Loss: 0.1789\n",
            "Epoch [1939/2500], Loss: 0.1789\n",
            "Epoch [1940/2500], Loss: 0.1789\n",
            "Epoch [1941/2500], Loss: 0.1789\n",
            "Epoch [1942/2500], Loss: 0.1789\n",
            "Epoch [1943/2500], Loss: 0.1790\n",
            "Epoch [1944/2500], Loss: 0.1792\n",
            "Epoch [1945/2500], Loss: 0.1795\n",
            "Epoch [1946/2500], Loss: 0.1802\n",
            "Epoch [1947/2500], Loss: 0.1813\n",
            "Epoch [1948/2500], Loss: 0.1836\n",
            "Epoch [1949/2500], Loss: 0.1868\n",
            "Epoch [1950/2500], Loss: 0.1928\n",
            "Epoch [1951/2500], Loss: 0.1975\n",
            "Epoch [1952/2500], Loss: 0.2038\n",
            "Epoch [1953/2500], Loss: 0.1983\n",
            "Epoch [1954/2500], Loss: 0.1915\n",
            "Epoch [1955/2500], Loss: 0.1817\n",
            "Epoch [1956/2500], Loss: 0.1791\n",
            "Epoch [1957/2500], Loss: 0.1828\n",
            "Epoch [1958/2500], Loss: 0.1875\n",
            "Epoch [1959/2500], Loss: 0.1893\n",
            "Epoch [1960/2500], Loss: 0.1850\n",
            "Epoch [1961/2500], Loss: 0.1811\n",
            "Epoch [1962/2500], Loss: 0.1791\n",
            "Epoch [1963/2500], Loss: 0.1805\n",
            "Epoch [1964/2500], Loss: 0.1828\n",
            "Epoch [1965/2500], Loss: 0.1836\n",
            "Epoch [1966/2500], Loss: 0.1827\n",
            "Epoch [1967/2500], Loss: 0.1802\n",
            "Epoch [1968/2500], Loss: 0.1789\n",
            "Epoch [1969/2500], Loss: 0.1788\n",
            "Epoch [1970/2500], Loss: 0.1799\n",
            "Epoch [1971/2500], Loss: 0.1807\n",
            "Epoch [1972/2500], Loss: 0.1805\n",
            "Epoch [1973/2500], Loss: 0.1797\n",
            "Epoch [1974/2500], Loss: 0.1786\n",
            "Epoch [1975/2500], Loss: 0.1782\n",
            "Epoch [1976/2500], Loss: 0.1783\n",
            "Epoch [1977/2500], Loss: 0.1787\n",
            "Epoch [1978/2500], Loss: 0.1788\n",
            "Epoch [1979/2500], Loss: 0.1786\n",
            "Epoch [1980/2500], Loss: 0.1780\n",
            "Epoch [1981/2500], Loss: 0.1776\n",
            "Epoch [1982/2500], Loss: 0.1774\n",
            "Epoch [1983/2500], Loss: 0.1775\n",
            "Epoch [1984/2500], Loss: 0.1777\n",
            "Epoch [1985/2500], Loss: 0.1778\n",
            "Epoch [1986/2500], Loss: 0.1779\n",
            "Epoch [1987/2500], Loss: 0.1776\n",
            "Epoch [1988/2500], Loss: 0.1773\n",
            "Epoch [1989/2500], Loss: 0.1769\n",
            "Epoch [1990/2500], Loss: 0.1766\n",
            "Epoch [1991/2500], Loss: 0.1764\n",
            "Epoch [1992/2500], Loss: 0.1764\n",
            "Epoch [1993/2500], Loss: 0.1766\n",
            "Epoch [1994/2500], Loss: 0.1766\n",
            "Epoch [1995/2500], Loss: 0.1767\n",
            "Epoch [1996/2500], Loss: 0.1766\n",
            "Epoch [1997/2500], Loss: 0.1764\n",
            "Epoch [1998/2500], Loss: 0.1763\n",
            "Epoch [1999/2500], Loss: 0.1762\n",
            "Epoch [2000/2500], Loss: 0.1761\n",
            "Epoch [2001/2500], Loss: 0.1761\n",
            "Epoch [2002/2500], Loss: 0.1761\n",
            "Epoch [2003/2500], Loss: 0.1762\n",
            "Epoch [2004/2500], Loss: 0.1764\n",
            "Epoch [2005/2500], Loss: 0.1767\n",
            "Epoch [2006/2500], Loss: 0.1772\n",
            "Epoch [2007/2500], Loss: 0.1779\n",
            "Epoch [2008/2500], Loss: 0.1789\n",
            "Epoch [2009/2500], Loss: 0.1805\n",
            "Epoch [2010/2500], Loss: 0.1825\n",
            "Epoch [2011/2500], Loss: 0.1858\n",
            "Epoch [2012/2500], Loss: 0.1879\n",
            "Epoch [2013/2500], Loss: 0.1913\n",
            "Epoch [2014/2500], Loss: 0.1888\n",
            "Epoch [2015/2500], Loss: 0.1875\n",
            "Epoch [2016/2500], Loss: 0.1811\n",
            "Epoch [2017/2500], Loss: 0.1774\n",
            "Epoch [2018/2500], Loss: 0.1758\n",
            "Epoch [2019/2500], Loss: 0.1770\n",
            "Epoch [2020/2500], Loss: 0.1798\n",
            "Epoch [2021/2500], Loss: 0.1816\n",
            "Epoch [2022/2500], Loss: 0.1831\n",
            "Epoch [2023/2500], Loss: 0.1807\n",
            "Epoch [2024/2500], Loss: 0.1786\n",
            "Epoch [2025/2500], Loss: 0.1760\n",
            "Epoch [2026/2500], Loss: 0.1749\n",
            "Epoch [2027/2500], Loss: 0.1752\n",
            "Epoch [2028/2500], Loss: 0.1763\n",
            "Epoch [2029/2500], Loss: 0.1778\n",
            "Epoch [2030/2500], Loss: 0.1783\n",
            "Epoch [2031/2500], Loss: 0.1786\n",
            "Epoch [2032/2500], Loss: 0.1777\n",
            "Epoch [2033/2500], Loss: 0.1768\n",
            "Epoch [2034/2500], Loss: 0.1758\n",
            "Epoch [2035/2500], Loss: 0.1751\n",
            "Epoch [2036/2500], Loss: 0.1748\n",
            "Epoch [2037/2500], Loss: 0.1747\n",
            "Epoch [2038/2500], Loss: 0.1748\n",
            "Epoch [2039/2500], Loss: 0.1749\n",
            "Epoch [2040/2500], Loss: 0.1751\n",
            "Epoch [2041/2500], Loss: 0.1753\n",
            "Epoch [2042/2500], Loss: 0.1755\n",
            "Epoch [2043/2500], Loss: 0.1757\n",
            "Epoch [2044/2500], Loss: 0.1760\n",
            "Epoch [2045/2500], Loss: 0.1762\n",
            "Epoch [2046/2500], Loss: 0.1763\n",
            "Epoch [2047/2500], Loss: 0.1763\n",
            "Epoch [2048/2500], Loss: 0.1760\n",
            "Epoch [2049/2500], Loss: 0.1756\n",
            "Epoch [2050/2500], Loss: 0.1751\n",
            "Epoch [2051/2500], Loss: 0.1747\n",
            "Epoch [2052/2500], Loss: 0.1743\n",
            "Epoch [2053/2500], Loss: 0.1740\n",
            "Epoch [2054/2500], Loss: 0.1739\n",
            "Epoch [2055/2500], Loss: 0.1738\n",
            "Epoch [2056/2500], Loss: 0.1738\n",
            "Epoch [2057/2500], Loss: 0.1737\n",
            "Epoch [2058/2500], Loss: 0.1736\n",
            "Epoch [2059/2500], Loss: 0.1734\n",
            "Epoch [2060/2500], Loss: 0.1733\n",
            "Epoch [2061/2500], Loss: 0.1732\n",
            "Epoch [2062/2500], Loss: 0.1731\n",
            "Epoch [2063/2500], Loss: 0.1730\n",
            "Epoch [2064/2500], Loss: 0.1730\n",
            "Epoch [2065/2500], Loss: 0.1729\n",
            "Epoch [2066/2500], Loss: 0.1729\n",
            "Epoch [2067/2500], Loss: 0.1728\n",
            "Epoch [2068/2500], Loss: 0.1728\n",
            "Epoch [2069/2500], Loss: 0.1728\n",
            "Epoch [2070/2500], Loss: 0.1728\n",
            "Epoch [2071/2500], Loss: 0.1727\n",
            "Epoch [2072/2500], Loss: 0.1727\n",
            "Epoch [2073/2500], Loss: 0.1727\n",
            "Epoch [2074/2500], Loss: 0.1727\n",
            "Epoch [2075/2500], Loss: 0.1727\n",
            "Epoch [2076/2500], Loss: 0.1727\n",
            "Epoch [2077/2500], Loss: 0.1728\n",
            "Epoch [2078/2500], Loss: 0.1729\n",
            "Epoch [2079/2500], Loss: 0.1731\n",
            "Epoch [2080/2500], Loss: 0.1735\n",
            "Epoch [2081/2500], Loss: 0.1743\n",
            "Epoch [2082/2500], Loss: 0.1757\n",
            "Epoch [2083/2500], Loss: 0.1785\n",
            "Epoch [2084/2500], Loss: 0.1830\n",
            "Epoch [2085/2500], Loss: 0.1899\n",
            "Epoch [2086/2500], Loss: 0.1979\n",
            "Epoch [2087/2500], Loss: 0.1998\n",
            "Epoch [2088/2500], Loss: 0.2012\n",
            "Epoch [2089/2500], Loss: 0.1886\n",
            "Epoch [2090/2500], Loss: 0.1885\n",
            "Epoch [2091/2500], Loss: 0.1827\n",
            "Epoch [2092/2500], Loss: 0.1822\n",
            "Epoch [2093/2500], Loss: 0.1819\n",
            "Epoch [2094/2500], Loss: 0.1810\n",
            "Epoch [2095/2500], Loss: 0.1848\n",
            "Epoch [2096/2500], Loss: 0.1787\n",
            "Epoch [2097/2500], Loss: 0.1748\n",
            "Epoch [2098/2500], Loss: 0.1753\n",
            "Epoch [2099/2500], Loss: 0.1784\n",
            "Epoch [2100/2500], Loss: 0.1825\n",
            "Epoch [2101/2500], Loss: 0.1777\n",
            "Epoch [2102/2500], Loss: 0.1732\n",
            "Epoch [2103/2500], Loss: 0.1729\n",
            "Epoch [2104/2500], Loss: 0.1751\n",
            "Epoch [2105/2500], Loss: 0.1764\n",
            "Epoch [2106/2500], Loss: 0.1744\n",
            "Epoch [2107/2500], Loss: 0.1733\n",
            "Epoch [2108/2500], Loss: 0.1744\n",
            "Epoch [2109/2500], Loss: 0.1747\n",
            "Epoch [2110/2500], Loss: 0.1728\n",
            "Epoch [2111/2500], Loss: 0.1712\n",
            "Epoch [2112/2500], Loss: 0.1720\n",
            "Epoch [2113/2500], Loss: 0.1734\n",
            "Epoch [2114/2500], Loss: 0.1733\n",
            "Epoch [2115/2500], Loss: 0.1722\n",
            "Epoch [2116/2500], Loss: 0.1717\n",
            "Epoch [2117/2500], Loss: 0.1720\n",
            "Epoch [2118/2500], Loss: 0.1721\n",
            "Epoch [2119/2500], Loss: 0.1714\n",
            "Epoch [2120/2500], Loss: 0.1709\n",
            "Epoch [2121/2500], Loss: 0.1709\n",
            "Epoch [2122/2500], Loss: 0.1711\n",
            "Epoch [2123/2500], Loss: 0.1714\n",
            "Epoch [2124/2500], Loss: 0.1712\n",
            "Epoch [2125/2500], Loss: 0.1711\n",
            "Epoch [2126/2500], Loss: 0.1709\n",
            "Epoch [2127/2500], Loss: 0.1709\n",
            "Epoch [2128/2500], Loss: 0.1710\n",
            "Epoch [2129/2500], Loss: 0.1708\n",
            "Epoch [2130/2500], Loss: 0.1705\n",
            "Epoch [2131/2500], Loss: 0.1703\n",
            "Epoch [2132/2500], Loss: 0.1703\n",
            "Epoch [2133/2500], Loss: 0.1703\n",
            "Epoch [2134/2500], Loss: 0.1702\n",
            "Epoch [2135/2500], Loss: 0.1700\n",
            "Epoch [2136/2500], Loss: 0.1699\n",
            "Epoch [2137/2500], Loss: 0.1699\n",
            "Epoch [2138/2500], Loss: 0.1699\n",
            "Epoch [2139/2500], Loss: 0.1698\n",
            "Epoch [2140/2500], Loss: 0.1697\n",
            "Epoch [2141/2500], Loss: 0.1697\n",
            "Epoch [2142/2500], Loss: 0.1696\n",
            "Epoch [2143/2500], Loss: 0.1696\n",
            "Epoch [2144/2500], Loss: 0.1696\n",
            "Epoch [2145/2500], Loss: 0.1696\n",
            "Epoch [2146/2500], Loss: 0.1695\n",
            "Epoch [2147/2500], Loss: 0.1695\n",
            "Epoch [2148/2500], Loss: 0.1695\n",
            "Epoch [2149/2500], Loss: 0.1696\n",
            "Epoch [2150/2500], Loss: 0.1699\n",
            "Epoch [2151/2500], Loss: 0.1702\n",
            "Epoch [2152/2500], Loss: 0.1708\n",
            "Epoch [2153/2500], Loss: 0.1719\n",
            "Epoch [2154/2500], Loss: 0.1739\n",
            "Epoch [2155/2500], Loss: 0.1769\n",
            "Epoch [2156/2500], Loss: 0.1817\n",
            "Epoch [2157/2500], Loss: 0.1859\n",
            "Epoch [2158/2500], Loss: 0.1909\n",
            "Epoch [2159/2500], Loss: 0.1876\n",
            "Epoch [2160/2500], Loss: 0.1831\n",
            "Epoch [2161/2500], Loss: 0.1740\n",
            "Epoch [2162/2500], Loss: 0.1697\n",
            "Epoch [2163/2500], Loss: 0.1708\n",
            "Epoch [2164/2500], Loss: 0.1750\n",
            "Epoch [2165/2500], Loss: 0.1792\n",
            "Epoch [2166/2500], Loss: 0.1779\n",
            "Epoch [2167/2500], Loss: 0.1752\n",
            "Epoch [2168/2500], Loss: 0.1706\n",
            "Epoch [2169/2500], Loss: 0.1687\n",
            "Epoch [2170/2500], Loss: 0.1697\n",
            "Epoch [2171/2500], Loss: 0.1720\n",
            "Epoch [2172/2500], Loss: 0.1742\n",
            "Epoch [2173/2500], Loss: 0.1739\n",
            "Epoch [2174/2500], Loss: 0.1726\n",
            "Epoch [2175/2500], Loss: 0.1705\n",
            "Epoch [2176/2500], Loss: 0.1689\n",
            "Epoch [2177/2500], Loss: 0.1684\n",
            "Epoch [2178/2500], Loss: 0.1689\n",
            "Epoch [2179/2500], Loss: 0.1700\n",
            "Epoch [2180/2500], Loss: 0.1709\n",
            "Epoch [2181/2500], Loss: 0.1714\n",
            "Epoch [2182/2500], Loss: 0.1711\n",
            "Epoch [2183/2500], Loss: 0.1702\n",
            "Epoch [2184/2500], Loss: 0.1690\n",
            "Epoch [2185/2500], Loss: 0.1682\n",
            "Epoch [2186/2500], Loss: 0.1679\n",
            "Epoch [2187/2500], Loss: 0.1679\n",
            "Epoch [2188/2500], Loss: 0.1682\n",
            "Epoch [2189/2500], Loss: 0.1686\n",
            "Epoch [2190/2500], Loss: 0.1689\n",
            "Epoch [2191/2500], Loss: 0.1691\n",
            "Epoch [2192/2500], Loss: 0.1692\n",
            "Epoch [2193/2500], Loss: 0.1690\n",
            "Epoch [2194/2500], Loss: 0.1688\n",
            "Epoch [2195/2500], Loss: 0.1685\n",
            "Epoch [2196/2500], Loss: 0.1681\n",
            "Epoch [2197/2500], Loss: 0.1678\n",
            "Epoch [2198/2500], Loss: 0.1675\n",
            "Epoch [2199/2500], Loss: 0.1673\n",
            "Epoch [2200/2500], Loss: 0.1672\n",
            "Epoch [2201/2500], Loss: 0.1671\n",
            "Epoch [2202/2500], Loss: 0.1671\n",
            "Epoch [2203/2500], Loss: 0.1671\n",
            "Epoch [2204/2500], Loss: 0.1671\n",
            "Epoch [2205/2500], Loss: 0.1672\n",
            "Epoch [2206/2500], Loss: 0.1672\n",
            "Epoch [2207/2500], Loss: 0.1673\n",
            "Epoch [2208/2500], Loss: 0.1675\n",
            "Epoch [2209/2500], Loss: 0.1678\n",
            "Epoch [2210/2500], Loss: 0.1681\n",
            "Epoch [2211/2500], Loss: 0.1687\n",
            "Epoch [2212/2500], Loss: 0.1695\n",
            "Epoch [2213/2500], Loss: 0.1707\n",
            "Epoch [2214/2500], Loss: 0.1720\n",
            "Epoch [2215/2500], Loss: 0.1738\n",
            "Epoch [2216/2500], Loss: 0.1752\n",
            "Epoch [2217/2500], Loss: 0.1767\n",
            "Epoch [2218/2500], Loss: 0.1761\n",
            "Epoch [2219/2500], Loss: 0.1749\n",
            "Epoch [2220/2500], Loss: 0.1717\n",
            "Epoch [2221/2500], Loss: 0.1689\n",
            "Epoch [2222/2500], Loss: 0.1669\n",
            "Epoch [2223/2500], Loss: 0.1664\n",
            "Epoch [2224/2500], Loss: 0.1671\n",
            "Epoch [2225/2500], Loss: 0.1685\n",
            "Epoch [2226/2500], Loss: 0.1701\n",
            "Epoch [2227/2500], Loss: 0.1711\n",
            "Epoch [2228/2500], Loss: 0.1718\n",
            "Epoch [2229/2500], Loss: 0.1711\n",
            "Epoch [2230/2500], Loss: 0.1702\n",
            "Epoch [2231/2500], Loss: 0.1686\n",
            "Epoch [2232/2500], Loss: 0.1672\n",
            "Epoch [2233/2500], Loss: 0.1662\n",
            "Epoch [2234/2500], Loss: 0.1658\n",
            "Epoch [2235/2500], Loss: 0.1659\n",
            "Epoch [2236/2500], Loss: 0.1663\n",
            "Epoch [2237/2500], Loss: 0.1669\n",
            "Epoch [2238/2500], Loss: 0.1676\n",
            "Epoch [2239/2500], Loss: 0.1683\n",
            "Epoch [2240/2500], Loss: 0.1689\n",
            "Epoch [2241/2500], Loss: 0.1694\n",
            "Epoch [2242/2500], Loss: 0.1695\n",
            "Epoch [2243/2500], Loss: 0.1694\n",
            "Epoch [2244/2500], Loss: 0.1689\n",
            "Epoch [2245/2500], Loss: 0.1683\n",
            "Epoch [2246/2500], Loss: 0.1674\n",
            "Epoch [2247/2500], Loss: 0.1666\n",
            "Epoch [2248/2500], Loss: 0.1659\n",
            "Epoch [2249/2500], Loss: 0.1654\n",
            "Epoch [2250/2500], Loss: 0.1652\n",
            "Epoch [2251/2500], Loss: 0.1651\n",
            "Epoch [2252/2500], Loss: 0.1651\n",
            "Epoch [2253/2500], Loss: 0.1653\n",
            "Epoch [2254/2500], Loss: 0.1656\n",
            "Epoch [2255/2500], Loss: 0.1659\n",
            "Epoch [2256/2500], Loss: 0.1664\n",
            "Epoch [2257/2500], Loss: 0.1670\n",
            "Epoch [2258/2500], Loss: 0.1677\n",
            "Epoch [2259/2500], Loss: 0.1686\n",
            "Epoch [2260/2500], Loss: 0.1697\n",
            "Epoch [2261/2500], Loss: 0.1706\n",
            "Epoch [2262/2500], Loss: 0.1717\n",
            "Epoch [2263/2500], Loss: 0.1719\n",
            "Epoch [2264/2500], Loss: 0.1718\n",
            "Epoch [2265/2500], Loss: 0.1704\n",
            "Epoch [2266/2500], Loss: 0.1689\n",
            "Epoch [2267/2500], Loss: 0.1669\n",
            "Epoch [2268/2500], Loss: 0.1655\n",
            "Epoch [2269/2500], Loss: 0.1646\n",
            "Epoch [2270/2500], Loss: 0.1645\n",
            "Epoch [2271/2500], Loss: 0.1649\n",
            "Epoch [2272/2500], Loss: 0.1656\n",
            "Epoch [2273/2500], Loss: 0.1665\n",
            "Epoch [2274/2500], Loss: 0.1673\n",
            "Epoch [2275/2500], Loss: 0.1683\n",
            "Epoch [2276/2500], Loss: 0.1687\n",
            "Epoch [2277/2500], Loss: 0.1693\n",
            "Epoch [2278/2500], Loss: 0.1688\n",
            "Epoch [2279/2500], Loss: 0.1684\n",
            "Epoch [2280/2500], Loss: 0.1672\n",
            "Epoch [2281/2500], Loss: 0.1662\n",
            "Epoch [2282/2500], Loss: 0.1651\n",
            "Epoch [2283/2500], Loss: 0.1643\n",
            "Epoch [2284/2500], Loss: 0.1639\n",
            "Epoch [2285/2500], Loss: 0.1637\n",
            "Epoch [2286/2500], Loss: 0.1638\n",
            "Epoch [2287/2500], Loss: 0.1640\n",
            "Epoch [2288/2500], Loss: 0.1644\n",
            "Epoch [2289/2500], Loss: 0.1648\n",
            "Epoch [2290/2500], Loss: 0.1654\n",
            "Epoch [2291/2500], Loss: 0.1661\n",
            "Epoch [2292/2500], Loss: 0.1670\n",
            "Epoch [2293/2500], Loss: 0.1680\n",
            "Epoch [2294/2500], Loss: 0.1690\n",
            "Epoch [2295/2500], Loss: 0.1696\n",
            "Epoch [2296/2500], Loss: 0.1699\n",
            "Epoch [2297/2500], Loss: 0.1694\n",
            "Epoch [2298/2500], Loss: 0.1685\n",
            "Epoch [2299/2500], Loss: 0.1669\n",
            "Epoch [2300/2500], Loss: 0.1655\n",
            "Epoch [2301/2500], Loss: 0.1642\n",
            "Epoch [2302/2500], Loss: 0.1635\n",
            "Epoch [2303/2500], Loss: 0.1632\n",
            "Epoch [2304/2500], Loss: 0.1633\n",
            "Epoch [2305/2500], Loss: 0.1637\n",
            "Epoch [2306/2500], Loss: 0.1644\n",
            "Epoch [2307/2500], Loss: 0.1652\n",
            "Epoch [2308/2500], Loss: 0.1660\n",
            "Epoch [2309/2500], Loss: 0.1670\n",
            "Epoch [2310/2500], Loss: 0.1675\n",
            "Epoch [2311/2500], Loss: 0.1682\n",
            "Epoch [2312/2500], Loss: 0.1681\n",
            "Epoch [2313/2500], Loss: 0.1680\n",
            "Epoch [2314/2500], Loss: 0.1669\n",
            "Epoch [2315/2500], Loss: 0.1659\n",
            "Epoch [2316/2500], Loss: 0.1646\n",
            "Epoch [2317/2500], Loss: 0.1636\n",
            "Epoch [2318/2500], Loss: 0.1629\n",
            "Epoch [2319/2500], Loss: 0.1625\n",
            "Epoch [2320/2500], Loss: 0.1624\n",
            "Epoch [2321/2500], Loss: 0.1625\n",
            "Epoch [2322/2500], Loss: 0.1629\n",
            "Epoch [2323/2500], Loss: 0.1633\n",
            "Epoch [2324/2500], Loss: 0.1638\n",
            "Epoch [2325/2500], Loss: 0.1645\n",
            "Epoch [2326/2500], Loss: 0.1653\n",
            "Epoch [2327/2500], Loss: 0.1662\n",
            "Epoch [2328/2500], Loss: 0.1671\n",
            "Epoch [2329/2500], Loss: 0.1679\n",
            "Epoch [2330/2500], Loss: 0.1683\n",
            "Epoch [2331/2500], Loss: 0.1680\n",
            "Epoch [2332/2500], Loss: 0.1672\n",
            "Epoch [2333/2500], Loss: 0.1657\n",
            "Epoch [2334/2500], Loss: 0.1644\n",
            "Epoch [2335/2500], Loss: 0.1631\n",
            "Epoch [2336/2500], Loss: 0.1623\n",
            "Epoch [2337/2500], Loss: 0.1620\n",
            "Epoch [2338/2500], Loss: 0.1620\n",
            "Epoch [2339/2500], Loss: 0.1623\n",
            "Epoch [2340/2500], Loss: 0.1628\n",
            "Epoch [2341/2500], Loss: 0.1634\n",
            "Epoch [2342/2500], Loss: 0.1640\n",
            "Epoch [2343/2500], Loss: 0.1648\n",
            "Epoch [2344/2500], Loss: 0.1654\n",
            "Epoch [2345/2500], Loss: 0.1663\n",
            "Epoch [2346/2500], Loss: 0.1665\n",
            "Epoch [2347/2500], Loss: 0.1671\n",
            "Epoch [2348/2500], Loss: 0.1664\n",
            "Epoch [2349/2500], Loss: 0.1661\n",
            "Epoch [2350/2500], Loss: 0.1646\n",
            "Epoch [2351/2500], Loss: 0.1635\n",
            "Epoch [2352/2500], Loss: 0.1623\n",
            "Epoch [2353/2500], Loss: 0.1615\n",
            "Epoch [2354/2500], Loss: 0.1610\n",
            "Epoch [2355/2500], Loss: 0.1610\n",
            "Epoch [2356/2500], Loss: 0.1612\n",
            "Epoch [2357/2500], Loss: 0.1615\n",
            "Epoch [2358/2500], Loss: 0.1620\n",
            "Epoch [2359/2500], Loss: 0.1624\n",
            "Epoch [2360/2500], Loss: 0.1631\n",
            "Epoch [2361/2500], Loss: 0.1635\n",
            "Epoch [2362/2500], Loss: 0.1644\n",
            "Epoch [2363/2500], Loss: 0.1651\n",
            "Epoch [2364/2500], Loss: 0.1661\n",
            "Epoch [2365/2500], Loss: 0.1670\n",
            "Epoch [2366/2500], Loss: 0.1676\n",
            "Epoch [2367/2500], Loss: 0.1676\n",
            "Epoch [2368/2500], Loss: 0.1667\n",
            "Epoch [2369/2500], Loss: 0.1650\n",
            "Epoch [2370/2500], Loss: 0.1630\n",
            "Epoch [2371/2500], Loss: 0.1613\n",
            "Epoch [2372/2500], Loss: 0.1604\n",
            "Epoch [2373/2500], Loss: 0.1603\n",
            "Epoch [2374/2500], Loss: 0.1609\n",
            "Epoch [2375/2500], Loss: 0.1619\n",
            "Epoch [2376/2500], Loss: 0.1629\n",
            "Epoch [2377/2500], Loss: 0.1639\n",
            "Epoch [2378/2500], Loss: 0.1646\n",
            "Epoch [2379/2500], Loss: 0.1648\n",
            "Epoch [2380/2500], Loss: 0.1646\n",
            "Epoch [2381/2500], Loss: 0.1640\n",
            "Epoch [2382/2500], Loss: 0.1631\n",
            "Epoch [2383/2500], Loss: 0.1623\n",
            "Epoch [2384/2500], Loss: 0.1615\n",
            "Epoch [2385/2500], Loss: 0.1613\n",
            "Epoch [2386/2500], Loss: 0.1609\n",
            "Epoch [2387/2500], Loss: 0.1609\n",
            "Epoch [2388/2500], Loss: 0.1607\n",
            "Epoch [2389/2500], Loss: 0.1607\n",
            "Epoch [2390/2500], Loss: 0.1606\n",
            "Epoch [2391/2500], Loss: 0.1608\n",
            "Epoch [2392/2500], Loss: 0.1610\n",
            "Epoch [2393/2500], Loss: 0.1614\n",
            "Epoch [2394/2500], Loss: 0.1620\n",
            "Epoch [2395/2500], Loss: 0.1626\n",
            "Epoch [2396/2500], Loss: 0.1638\n",
            "Epoch [2397/2500], Loss: 0.1644\n",
            "Epoch [2398/2500], Loss: 0.1659\n",
            "Epoch [2399/2500], Loss: 0.1658\n",
            "Epoch [2400/2500], Loss: 0.1665\n",
            "Epoch [2401/2500], Loss: 0.1653\n",
            "Epoch [2402/2500], Loss: 0.1644\n",
            "Epoch [2403/2500], Loss: 0.1626\n",
            "Epoch [2404/2500], Loss: 0.1612\n",
            "Epoch [2405/2500], Loss: 0.1601\n",
            "Epoch [2406/2500], Loss: 0.1596\n",
            "Epoch [2407/2500], Loss: 0.1596\n",
            "Epoch [2408/2500], Loss: 0.1597\n",
            "Epoch [2409/2500], Loss: 0.1602\n",
            "Epoch [2410/2500], Loss: 0.1603\n",
            "Epoch [2411/2500], Loss: 0.1609\n",
            "Epoch [2412/2500], Loss: 0.1610\n",
            "Epoch [2413/2500], Loss: 0.1616\n",
            "Epoch [2414/2500], Loss: 0.1620\n",
            "Epoch [2415/2500], Loss: 0.1626\n",
            "Epoch [2416/2500], Loss: 0.1634\n",
            "Epoch [2417/2500], Loss: 0.1639\n",
            "Epoch [2418/2500], Loss: 0.1643\n",
            "Epoch [2419/2500], Loss: 0.1636\n",
            "Epoch [2420/2500], Loss: 0.1626\n",
            "Epoch [2421/2500], Loss: 0.1610\n",
            "Epoch [2422/2500], Loss: 0.1596\n",
            "Epoch [2423/2500], Loss: 0.1587\n",
            "Epoch [2424/2500], Loss: 0.1584\n",
            "Epoch [2425/2500], Loss: 0.1586\n",
            "Epoch [2426/2500], Loss: 0.1590\n",
            "Epoch [2427/2500], Loss: 0.1596\n",
            "Epoch [2428/2500], Loss: 0.1601\n",
            "Epoch [2429/2500], Loss: 0.1607\n",
            "Epoch [2430/2500], Loss: 0.1610\n",
            "Epoch [2431/2500], Loss: 0.1614\n",
            "Epoch [2432/2500], Loss: 0.1615\n",
            "Epoch [2433/2500], Loss: 0.1617\n",
            "Epoch [2434/2500], Loss: 0.1620\n",
            "Epoch [2435/2500], Loss: 0.1621\n",
            "Epoch [2436/2500], Loss: 0.1628\n",
            "Epoch [2437/2500], Loss: 0.1625\n",
            "Epoch [2438/2500], Loss: 0.1630\n",
            "Epoch [2439/2500], Loss: 0.1617\n",
            "Epoch [2440/2500], Loss: 0.1612\n",
            "Epoch [2441/2500], Loss: 0.1597\n",
            "Epoch [2442/2500], Loss: 0.1587\n",
            "Epoch [2443/2500], Loss: 0.1580\n",
            "Epoch [2444/2500], Loss: 0.1579\n",
            "Epoch [2445/2500], Loss: 0.1580\n",
            "Epoch [2446/2500], Loss: 0.1582\n",
            "Epoch [2447/2500], Loss: 0.1585\n",
            "Epoch [2448/2500], Loss: 0.1585\n",
            "Epoch [2449/2500], Loss: 0.1585\n",
            "Epoch [2450/2500], Loss: 0.1582\n",
            "Epoch [2451/2500], Loss: 0.1581\n",
            "Epoch [2452/2500], Loss: 0.1580\n",
            "Epoch [2453/2500], Loss: 0.1581\n",
            "Epoch [2454/2500], Loss: 0.1586\n",
            "Epoch [2455/2500], Loss: 0.1593\n",
            "Epoch [2456/2500], Loss: 0.1606\n",
            "Epoch [2457/2500], Loss: 0.1621\n",
            "Epoch [2458/2500], Loss: 0.1642\n",
            "Epoch [2459/2500], Loss: 0.1660\n",
            "Epoch [2460/2500], Loss: 0.1677\n",
            "Epoch [2461/2500], Loss: 0.1683\n",
            "Epoch [2462/2500], Loss: 0.1669\n",
            "Epoch [2463/2500], Loss: 0.1655\n",
            "Epoch [2464/2500], Loss: 0.1623\n",
            "Epoch [2465/2500], Loss: 0.1614\n",
            "Epoch [2466/2500], Loss: 0.1597\n",
            "Epoch [2467/2500], Loss: 0.1595\n",
            "Epoch [2468/2500], Loss: 0.1594\n",
            "Epoch [2469/2500], Loss: 0.1598\n",
            "Epoch [2470/2500], Loss: 0.1607\n",
            "Epoch [2471/2500], Loss: 0.1615\n",
            "Epoch [2472/2500], Loss: 0.1629\n",
            "Epoch [2473/2500], Loss: 0.1621\n",
            "Epoch [2474/2500], Loss: 0.1617\n",
            "Epoch [2475/2500], Loss: 0.1592\n",
            "Epoch [2476/2500], Loss: 0.1575\n",
            "Epoch [2477/2500], Loss: 0.1565\n",
            "Epoch [2478/2500], Loss: 0.1565\n",
            "Epoch [2479/2500], Loss: 0.1573\n",
            "Epoch [2480/2500], Loss: 0.1580\n",
            "Epoch [2481/2500], Loss: 0.1586\n",
            "Epoch [2482/2500], Loss: 0.1584\n",
            "Epoch [2483/2500], Loss: 0.1585\n",
            "Epoch [2484/2500], Loss: 0.1584\n",
            "Epoch [2485/2500], Loss: 0.1586\n",
            "Epoch [2486/2500], Loss: 0.1589\n",
            "Epoch [2487/2500], Loss: 0.1591\n",
            "Epoch [2488/2500], Loss: 0.1592\n",
            "Epoch [2489/2500], Loss: 0.1587\n",
            "Epoch [2490/2500], Loss: 0.1581\n",
            "Epoch [2491/2500], Loss: 0.1572\n",
            "Epoch [2492/2500], Loss: 0.1566\n",
            "Epoch [2493/2500], Loss: 0.1563\n",
            "Epoch [2494/2500], Loss: 0.1561\n",
            "Epoch [2495/2500], Loss: 0.1562\n",
            "Epoch [2496/2500], Loss: 0.1561\n",
            "Epoch [2497/2500], Loss: 0.1560\n",
            "Epoch [2498/2500], Loss: 0.1558\n",
            "Epoch [2499/2500], Loss: 0.1556\n",
            "Epoch [2500/2500], Loss: 0.1556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.91      0.93     14234\n",
            "         1.0       0.91      0.94      0.93     13976\n",
            "\n",
            "    accuracy                           0.93     28210\n",
            "   macro avg       0.93      0.93      0.93     28210\n",
            "weighted avg       0.93      0.93      0.93     28210\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization\n",
        "        self.relu1 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)  # Batch normalization\n",
        "        self.relu2 = nn.Tanh()\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)  # Batch normalization\n",
        "        self.relu3 = nn.Tanh()\n",
        "        self.fc4 = nn.Linear(32, 1)  # Output layer for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Apply He initialization to weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='tanh')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)  # Batch normalization\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)  # Batch normalization\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)  # Batch normalization\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)  # Reshape for binary classification\n",
        "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
        "input_size = X_train_tensor.shape[1]  # Number of features\n",
        "\n",
        "# Instantiate the model\n",
        "model = Net(input_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the GPU\n",
        "model.to(device)\n",
        "\n",
        "# Move the data to the GPU\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "y_train_tensor = y_train_tensor.to(device)\n",
        "X_test_tensor = X_test_tensor.to(device)\n",
        "y_test_tensor = y_test_tensor.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 2500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_prob = model(X_test_tensor)\n",
        "    y_pred = (y_pred_prob > 0.5).float()  # Convert probabilities to binary predictions\n",
        "\n",
        "    # Move tensors back to CPU for classification_report\n",
        "    y_pred_cpu = y_pred.cpu()\n",
        "    y_test_tensor_cpu = y_test_tensor.cpu()\n",
        "\n",
        "    print(classification_report(y_test_tensor_cpu, y_pred_cpu))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Ep7HKgrNf67y"
      },
      "outputs": [],
      "source": [
        "validation_data=pd.read_csv(\"C:\\\\Users\\\\UseR\\\\Downloads\\\\validation_data_to_be_shared.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(41792, 20)\n"
          ]
        }
      ],
      "source": [
        "df = preprocess_and_feature_engineering(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "account_number           0\n",
              "onus_attribute_1     10990\n",
              "total_enquiries          0\n",
              "avg_enquiries         1127\n",
              "max_enquiries         1127\n",
              "zero_periods             0\n",
              "std_dev_enquiries     1127\n",
              "bureau_01                0\n",
              "bureau_02                0\n",
              "bureau_03                0\n",
              "bureau_04                0\n",
              "bureau_05                0\n",
              "onus_attribute_01        0\n",
              "onus_attribute_02        0\n",
              "onus_attribute_03        0\n",
              "onus_attribute_04        0\n",
              "transaction_col1     10990\n",
              "transaction_col2     10990\n",
              "transaction_col3     10990\n",
              "transaction_col4         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df = df.dropna(subset=['onus_attribute_1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate the account_number column before scaling\n",
        "acc_no = df_imputed['account_number']\n",
        "\n",
        "# Drop account_number from df_imputed as it's not part of the features for scaling\n",
        "df_imputed_features = df_imputed.drop(\"account_number\", axis=1)\n",
        "\n",
        "# Apply the scaler to the remaining features\n",
        "df_imputed_scaled = scaler.transform(df_imputed_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming `model` is the trained PyTorch model and `df_imputed_scaled` is the new data for prediction\n",
        "\n",
        "# Convert new data (NumPy array) to PyTorch tensor\n",
        "df_tensor = torch.tensor(df_imputed_scaled, dtype=torch.float32)\n",
        "\n",
        "# Move the tensor to the same device as the model\n",
        "df_tensor = df_tensor.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    # Get the probabilities directly from the model\n",
        "    y_prob_val = model(df_tensor).cpu().numpy()  # Move probabilities back to CPU and convert to NumPy\n",
        "\n",
        "# `y_prob_val` contains the probabilities of the positive class (e.g., bad_flag=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation predictions have been calculated and saved to 'validation_predictions.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten the predicted probabilities to 1D\n",
        "y_prob_val = y_prob_val.flatten()\n",
        "\n",
        "# Assuming `acc_no` is the list of account numbers corresponding to the validation data\n",
        "validation_results = pd.DataFrame({\n",
        "    \"account_number\": acc_no,  # Account numbers corresponding to validation data\n",
        "    \"predicted_probability\": y_prob_val  # Flattened predicted probabilities\n",
        "})\n",
        "\n",
        "# Save the results to a CSV file\n",
        "validation_results.to_csv(\"validation_predictions.csv\", index=False)\n",
        "print(\"Validation predictions have been calculated and saved to 'validation_predictions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_out=pd.read_csv(\"validation_data_pred.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        100001.0\n",
              "1        100002.0\n",
              "2        100003.0\n",
              "3        100004.0\n",
              "4        100005.0\n",
              "           ...   \n",
              "41787    141788.0\n",
              "41788    141789.0\n",
              "41789    141790.0\n",
              "41790    141791.0\n",
              "41791    141792.0\n",
              "Name: account_number, Length: 41792, dtype: float64"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_out['account_number']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
